{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wk12 Demo - Decision Trees\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2019`__\n",
    "\n",
    "This week we'll be looking at Decision Trees\n",
    "\n",
    "In class today we'll start out by reviewing the Decision Tree algorithm. We'll look at Regression and Classification trees, learning, pruning, and evluation. We'll extend our discussion to Ensemble methods, including Random Forests, Bagging, and Boosting.  \n",
    "\n",
    "By the end of today's demo you should be able to:  \n",
    "* ... __describe__ Decision Tree CART algorithm  \n",
    "* ... __identify__ Assumptions/constraints for learning DTs\n",
    "* ... __explain__ The difference between regression trees and classification trees\n",
    "* ... __explain__ The difference between bagging, RF, and boosting\n",
    "* ... __describe__ The PLANET method for distributing DT learning\n",
    "\n",
    "\n",
    "__`Additional Resources:`__    \n",
    "Chapter 9.2 ESL (or ISL Chapter 8) - Tree-Based Methods    \n",
    "https://explained.ai/decision-tree-viz/index.html - How to visualize decision trees      \n",
    "https://www.youtube.com/watch?v=iOucwX7Z1HU \"Wisdom of the crowd\" (jelly beans)      \n",
    "https://explained.ai/gradient-boosting/index.html - Gradient Booted Models (GBMs) explained   \n",
    "https://statweb.stanford.edu/~jhf/ftp/trebst.pdf - Greedy Function Approximation - a Gradient Boosting Machine  \n",
    "https://statweb.stanford.edu/~jhf/ftp/stobst.pdf - Stochastic Gradient Boosting   \n",
    "https://xgboost.readthedocs.io/en/latest/tutorials/model.html XGBoost Docs    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Decision Tree Review \n",
    "*Based on ESL Chapter 9.2 - Tree Based Methods*\n",
    "\n",
    "## Benefits\n",
    "* One of the most popular approaches to ML in  practice\n",
    "* Can handle numeric, categorical, and ordinal  features\n",
    "* No preprocessing required, no standardization/scaling\n",
    "* Handles Missing values naturally\n",
    "* NAs do not affect performance metrics\n",
    "* Interaction features\n",
    "* Highly Scalable\n",
    "* Variable selection\n",
    "* Excellent performance on a variety of problems\n",
    "* Off the shelf with very few hyperparameters\n",
    "\n",
    "\n",
    "## Approach\n",
    "* A decision tree represents a hierarchical  segmentation of the data\n",
    "* The original segment is called the root node and is the entire data set\n",
    "* The root node is partitioned into two or more segments by applying a series of simple rules  over input variables\n",
    "* For example, `risk == low`, vs `risk == not low`\n",
    "* Each rule assigns the observations to a segment based on its  input value\n",
    "* Each resulting segment can be further  partitioned into sub-segments, and so  on\n",
    "* For example `risk == low` can be partitioned into  `income == low` and `income == not low`\n",
    "* The segments are also called nodes,  and the final segments are called leaf  nodes or leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig9.2-ESL-tree-diagrams.png\">\n",
    "\n",
    "*Partitions and CART. Left panel shows a partition of a\n",
    "two-dimensional feature space by recursive binary splitting, as used in CART,\n",
    "applied to some fake data. Middle panel shows the tree corresponding\n",
    "to the partition in the left panel, and a perspective plot of the\n",
    "prediction surface appears in the right panel.* __Based on FIGURE 9.2 Elements of Statistical Learning.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART - Classification And Regression Trees \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "### How do we grow a regression tree?\n",
    "\n",
    "Our data consists of $N$ observations with $p$ features. Suppose we partition the data into $M$ regions $R_1, R_2,...,R_M$ , and model the response as a constant $c_m$ in each region.\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{m=1}^{M}c_mI(x \\in R_m) \\tag{9.10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sum of squares criterion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum(y_i - f(x_i))^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction $\\hat c_m$ is just the average of $y_i$ in region $R_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat c_m = avg(y_i|x_i \\in R_m) \\tag{9.11}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DISCUSSION__ \n",
    "\n",
    "* Can we find the best binary partition in terms of minimum sum of squares? \n",
    "* What is the big $O$ complexity of this problem?\n",
    "* What is another criterion often used for regression tree partitioining?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since finding the best binary partition in terms of minimum sum of squares is computationaly infeasable, we proceed with a greedy algorithm\n",
    "Starting with all of the data, consider a splitting variable $j$ and a split point $s$, we define the pair of half planes:\n",
    "\n",
    "$$\n",
    "R_1(j,s) = \\{X|X_j \\leq s\\} \\text{ and } R_2(j,s) = \\{X|X_j \\gt s\\} \\tag{9.12}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek the splitting variable $j$ and split point $s$ that solve\n",
    "\n",
    "$$\n",
    "\\min_{j,s}[\\min_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i - c_1)^2 + \\min_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i - c_2)^2] \\tag{9.13}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any choice $j$ and $s$, the inner minimization is solved by\n",
    "\n",
    "$$\n",
    "\\hat c_1 = avg(y_i|x_i \\in R_1(j,s)) \\text{ and } \\hat c_2 = avg(y_i|x_i \\in R_2(j,s)) \\tag{9.14}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each splitting variable, the determination of the split point $s$ can be done very quickly and hence by scanning through all of the inputs, determination of the best pair $(j, s)$ is feasible.\n",
    "\n",
    "Having found the best split, we partition the data into the two resulting regions and repeat the splitting process on each of the two regions. Then this process is repeated on all of the resulting regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DISCUSSION__ \n",
    "\n",
    "Consider three types of variables: continuous, ordered (ex. ratings), and categorical\n",
    "* How many split points will we have? \n",
    "* Notice that in equation 9.12, we split our data based on whether it is smaller or larger than our split point $s$. How can we find split points for categorical variables (ie, variables which are not ordered)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brieman's theorem\n",
    "\n",
    "For unordered domains, there are ${p \\choose 2}$ possible splits, where $p$ is the number of categories.   \n",
    "\n",
    "Breiman presents an algorithm for finding the best split predicate for a categorical attribute without evaluating\n",
    "all possible subsets of $p$, based on the observation that the optimal split predicate is a subsequence in the list of values for $p_i$ sorted by the average $y$ value.\n",
    "\n",
    "<!-- <img src=\"brieman.png\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1\n",
    "Run the code cells below, and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DATASET: Run this cell as is\n",
    "import pandas as pd\n",
    "\n",
    "x = [\"c\",\"b\",\"b\",\"c\",\"a\",\"b\",\"a\"]\n",
    "y = [0.8,0.9,1.4,0.6,3.2,2.5,3.0]\n",
    "    \n",
    "\n",
    "df = pd.DataFrame([x,y]).transpose()\n",
    "df.columns = ['x', 'y']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET MEAN VALUES OF Y : Run this cell as is\n",
    "df[\"y\"] = pd.to_numeric(df[\"y\"])\n",
    "mean_y = df.groupby('x').mean().reset_index().sort_values(by=['y'])\n",
    "mean_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DISCUSSION__\n",
    "* How many possible split points are there to start with?\n",
    "* How many possible split points are there using Brieman's method? List the splits.\n",
    "* How large should we grow the tree? What are the tradeoffs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-Complexity Pruning \n",
    "ESL, pg. 308"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preferred strategy is to grow a large tree $T_0$, stopping the splitting\n",
    "process only when some minimum node size (say 5) is reached. Then this\n",
    "large tree is pruned using cost-complexity pruning, which we now describe.\n",
    "We define a subtree $T ⊂ T_0$ to be any tree that can be obtained by\n",
    "pruning $T_0$, that is, collapsing any number of its internal (non-terminal)\n",
    "nodes. We index terminal nodes by $m$, with node $m$ representing region\n",
    "$R_m$. Let $|T|$ denote the number of terminal nodes in $T$. Letting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_m = \\#\\{x_i \\in R_m\\},\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat c_m = \\frac{1}{N_m} \\sum_{x_i \\in R_m} y_i, \\tag{9.15}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_m(T) = \\frac{1}{N_m}\\sum_{x_i \\in R_m} (y_i - \\hat c_m)^2),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define the *cost complexity criterion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C_\\alpha (T) = \\sum_{m=1}^{|T|} N_mQ_m(T) + \\alpha|T|. \\tag{9.16}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to find, for each $α$, the subtree $Tα ⊆ T_0$ to minimize $C_α(T)$.\n",
    "The tuning parameter $α ≥ 0$ governs the tradeoff between tree size and its\n",
    "goodness of fit to the data. Large values of $α$ result in smaller trees $T_α$, and\n",
    "conversely for smaller values of $α$. As the notation suggests, with $α = 0$ the\n",
    "solution is the full tree $T_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each α one can show that there is a unique smallest subtree $T_α$ that\n",
    "minimizes $C_α(T)$. To find $T_α$ we use *weakest link pruning*: we successively\n",
    "collapse the internal node that produces the smallest per-node increase in\n",
    "$m$ $N_mQ_m(T)$, and continue until we produce the single-node (root) tree.\n",
    "This gives a (finite) sequence of subtrees, and one can show this sequence\n",
    "must contain Tα. See Breiman et al. (1984) or Ripley (1996) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation of α is achieved by five- or tenfold cross-validation: we choose\n",
    "the value $\\hat α$ to minimize the cross-validated sum of squares. Our final tree\n",
    "is $T \\hat α$. See Breiman et al. (1984) or Ripley (1996) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2:\n",
    "Implement cost complexity pruning using a simple regression tree example. You may need to increase the size of this example to see meaningful results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a. Generate some random data for regression (RUN THIS CELL AS IS)\n",
    "# You may need to increase the n_samples and n_features to see meaningful results\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from matplotlib import pyplot\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=18, n_features=4 , noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. Fit an \"example\" tree using sklearn (RUN THIS CELL AS IS)\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeRegressor(max_leaf_nodes=5)\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need this if you are running this notebook in the w261 docker environment. \n",
    "# It takes a while to load everything - could be up to an hour depending on your network, etc..\n",
    "\n",
    "#!conda install -y python-graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT THIS\n",
    "#!conda install -y pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT THIS\n",
    "#!conda install -y graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c. Visualize the tree (RUN THIS CELL AS IS)\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None, node_ids=True) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuples of node ids and the values assinged to each.\n",
    "for i in sorted(zip(clf.apply(X),y)):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For this small example, you may want to do one iteration in pencil and paper before implementing in code.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT IMAGE OF HAND CALCULATIONS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### YOUR CODE HERE ##################\n",
    "\n",
    "############### (END) YOUR CODE #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BONUS: Visualize the trees at each iteration of the CCP algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees\n",
    "\n",
    "__DISCUSSION__\n",
    "* What modifications do we need to make for classification trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"purity-functions.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__FIGURE 9.3.__ *Node impurity measures for two-class classification, as a function\n",
    "of the proportion p in class 2. Cross-entropy has been scaled to pass through\n",
    "(0.5, 0.5).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a node $m$, representing a region $R_m$ with $N_m$ observations, let"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"class-proportion.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the proportion of class $k$ observations in node $m$. We classify the observations\n",
    "in node $m$ to class $k(m) = arg max_k \\hat p_{mk}$, the majority class in\n",
    "node $m$. Different measures $Q_m(T)$ of node impurity include the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"purity-equations.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three are similar, but crossentropy and the Gini index are differentiable, and hence more amenable to numerical optimization. \n",
    "\n",
    "In addition, cross-entropy and the Gini index are more sensitive to changes\n",
    "in the node probabilities than the misclassification rate. For example, in\n",
    "a two-class problem with 400 observations in each class (denote this by\n",
    "(400, 400)), suppose one split created nodes (300, 100) and (100, 300), while\n",
    "the other created nodes (200, 400) and (200, 0). Both splits produce a misclassification\n",
    "rate of 0.25, but the second split produces a pure node and is\n",
    "probably preferable. Both the Gini index and cross-entropy are lower for the\n",
    "second split. For this reason, either the Gini index or cross-entropy should\n",
    "be used when growing the tree. \n",
    "\n",
    "To guide cost-complexity pruning, any of the three measures can be used, but typically it is the misclassification rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Why binary splits?__\n",
    "Rather than splitting each node into just two groups at each stage (as\n",
    "above), we might consider multiway splits into more than two groups. While\n",
    "this can sometimes be useful, it is not a good general strategy. The problem\n",
    "is that multiway splits fragment the data too quickly, leaving insufficient\n",
    "data at the next level down. Hence we would want to use such splits only\n",
    "when needed. Since multiway splits can be achieved by a series of binary\n",
    "splits, the latter are preferred.\n",
    "\n",
    "* __Missing Predictor Values__\n",
    "See ESL p.311 - Surrogate predictors and split points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Distributed Tree Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLANET: Massively Parallel Learning of Tree Ensembles with MapReduce\n",
    "https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36296.pdf   \n",
    "\n",
    "\n",
    "The greedy tree induction algorithm we have described\n",
    "is simple and works well in practice. However, it does not\n",
    "scale well to large training datasets. FindBestSplit requires\n",
    "a full scan of the node’s input data, which can be large at\n",
    "higher levels of the tree. Large inputs that do not fit in main\n",
    "memory become a bottleneck because of the cost of scanning\n",
    "data from secondary storage. Even at lower levels of the tree\n",
    "where a node’s input dataset D is typically much smaller\n",
    "than D, loading D into memory still requires reading and\n",
    "writing partitions of D to secondary storage multiple times.\n",
    "\n",
    "PLANET uses MapReduce to distribute and scale tree induction to very large datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__: Step by Step PLANET \n",
    "[See slides for now](https://docs.google.com/presentation/d/1Womuq5YmCNfvRZceguNjettzK0_hh3XojIanrqZ_auQ/edit#slide=id.g2b2b939f7f_1_244)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Ensembles and gradient boosting\n",
    "\n",
    "The basic idea with ensembles is that several independent models can be combined to make a better model.  One example of this comes from the Netflix prize where several of the top competitors for the million dollar prize joined forced and averaged their models to produce a superior model.  There were a handful of competitors and that was enough improvement to put them into a tie for the best results.  \n",
    "\n",
    "Generally, ensembles include many more than a handful of fully trained models.  The package guidance for the R package gbm (gradient boosting machine) suggests using 3000 models, for example.  How can that many different models be generated?  All of the models need to be solving more or less the same problem.  You can't do them by hand.  You need a systematic method for generating these models.  We'll look quickly at two different methods.  \n",
    "\n",
    "## Independent methods\n",
    "Build individual trees independently\n",
    "\n",
    "### Bagging & Random Forests\n",
    "First we'll look at Bootstrap Aggregation (called bagging).  That was invented by late Professor Leo Breiman, the famous Berkeley statistician.  Professor Breiman invented bagging to deal with the well known high-variance of binary decision trees.  Here's how it works.  \n",
    "\n",
    "Generate a multitude of different training sets for the same problem.  Train a binary decision tree for each training set and average the results.  To generate multiple training sets take a random sample of the data.  The nominal formula for generating random training sets is to take a sample whose size is 50% of the original data set and extract the data from the original by sample with replacement.  Here's are some simple example to illustrate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data generator.  \n",
    "import numpy as np \n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def EnsembleDataGen(npts, stdDev):\n",
    "    #Define data set:\n",
    "    #Starter is X sampled regularly in [-10, 10], Y = X + noise\n",
    "    #Try swapping Y = X + noise for Y = np.sin(X) + noise\n",
    "    X = np.linspace(-10.0, 10.0, npts)\n",
    "    Y = X + np.random.normal(0.0, stdDev, npts)\n",
    "    #Y = np.sin(X) + np.random.normal(0.0, stdDev, npts)\n",
    "    return X.reshape([-1,1]), Y.reshape([-1,1])\n",
    "\n",
    "X,Y = EnsembleDataGen(100, 1.0)\n",
    "plt.scatter(X,Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building trees for bagging\n",
    "The core idea for bagging is to build high variance (complex) trees and then overcome the high-variance by average.  With binary decision trees the depth controls complexity (and variance).  So the trees for bagging are deeper than you might train if you were only building one tree and were trying to do the best trade off between bias and variance for a single tree.  One of the benefits of ensemble methods is that they don't require quite as much fussing with regularization parameters as single trees.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Take random samples with replacement, build trees for each one and average.  \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#define weighted sum function to accumulate - (function currying)\n",
    "def wt_sum_fcn(f1, f2, wt):\n",
    "    def wsf(x):\n",
    "        return (f1(x) + wt * f2(x))\n",
    "    return wsf\n",
    "\n",
    "def Bagging(nTrees, nDepth, bagFrac, X, Y):\n",
    "    \"\"\"\n",
    "    nTrees - number of trees in ensemble\n",
    "    nDepts - max depth of trees\n",
    "    bagFrac - fractional size of bags relative to full data set\n",
    "    X, Y - features, labels\n",
    "    \n",
    "    Return: Prediction function that is average of prediction functions of trees in ensemble\n",
    "    \"\"\"\n",
    "    nDataPts = len(X)\n",
    "    wt = float(1.0 / nTrees)\n",
    "    nSamp = int(bagFrac * nDataPts)\n",
    "        \n",
    "    #Define function T to accumulate average prediction functions from trained trees.  \n",
    "    #initialize T to fcn mapping all x to zero to start \n",
    "    T = lambda x: 0.0\n",
    "    \n",
    "    #loop to generate individual trees in ensemble\n",
    "    for i in range(nTrees):\n",
    "        \n",
    "        #take a random sample from the data\n",
    "        sampIdx = np.random.choice(nDataPts, nSamp)\n",
    "        xTrain = X[sampIdx]\n",
    "        yTrain = Y[sampIdx]\n",
    "        \n",
    "        #build a tree on the sampled data\n",
    "        tree = DecisionTreeRegressor(max_depth=nDepth)\n",
    "        tree.fit(xTrain, yTrain)\n",
    "        \n",
    "        #Add the new tree with a weight\n",
    "        T = wt_sum_fcn(T, tree.predict, wt)\n",
    "    return T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrees = 3  #try changing the number of trees being built\n",
    "nDepth = 6   #fairly deep for 100 data points\n",
    "bagFrac = 0.5   #Bag fraction - how many points in each of the random subsamples.  \n",
    "\n",
    "bag = Bagging(nTrees, nDepth, bagFrac, X, Y)\n",
    "\n",
    "result = bag(X)\n",
    "\n",
    "plt.plot(X, result, 'r')\n",
    "plt.scatter(X,Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some things to try\n",
    "1.  Change the number of trees in the ensemble through a range of values 1, 3, 5, 10.  Notice how the prediction smooths out.  \n",
    "2.  Change the trees to depth 1 trees.  What happens as you put more and more trees into the ensemble?  This is a good example showing that no amount of averaging will overcome a bias error.  This is why it's important to grow deep trees for bagging.  \n",
    "2.  In the code generator there's a suggestion to change the dependence of Y on X into a sinusoid.  Make that change and try some values for tree depth, number of trees in the ensemble to see what effect it has.  Also change the number of points in the data set and see what's required to get a relatively smooth fit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is similar to bagging in that we take sub-samples of the data to train individual trees and combine the results to form the final prediction. At a high level, the difference is the sampling method. RF builds trees on subsets of the features (columns), while bagging builds trees on subsets of the data (rows).    \n",
    "\n",
    "In reality we often implement a combination of bagging and RF where we take a sample of data and features.\n",
    "\n",
    "\n",
    "__Some considerations for building ensembles__:\n",
    "* __Tree diversity__ - Creating an ensemble in which each classifier is as different as possible\n",
    "while still being consistent with the training set is theoretically known to be\n",
    "an important feature for obtaining improved ensemble performance.\n",
    "\n",
    "* __Sub-sample distribution__ - Preprocess using K-means to insure that each tree contains some data points from each cluster, thus the individual trees distributions are similar to the original full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination Methods for ensembles\n",
    "\n",
    "* Majority Voting\n",
    "* Performance Weighting\n",
    "* etc..\n",
    "\n",
    "See also *Data Mining with Decision Trees: Theory and Applications; Lior Rokach and Oded Maimon* Chapter 7.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent methods\n",
    "Build trees sequentially which optimize for the error in predictions from the previous iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "Gradient boosting operates on a different principle from bagging.  The principle is easiest to explain for a regression problem like the one you just saw for bagging.  The idea with gradient boosting is that you fit a tree to the problem, then generate predicitons with that tree and subtract a small amount of the tree's prediction from the original regression labels.  Then the next tree gets trained on the leftovers.  \n",
    "\n",
    "See also: https://explained.ai/gradient-boosting/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def GradientBoosting(nTrees, nDepth, gamma, bagFrac, X, Y):\n",
    "    nDataPts = len(X)\n",
    "    nSamp = int(bagFrac * nDataPts)\n",
    "    \n",
    "    # Define function T to accumulate average prediction functions from trained trees.  \n",
    "    # initialize T to fcn mapping all x to zero to start \n",
    "    T = lambda x: 0.0\n",
    "    \n",
    "    # loop to generate individual trees in ensemble\n",
    "    for i in range(nTrees):\n",
    "        \n",
    "        # take a random sample from the data\n",
    "        sampIdx = np.random.choice(nDataPts, nSamp)\n",
    "        \n",
    "        xTrain = X[sampIdx]\n",
    "        \n",
    "        # estimate the regression values with the current trees.  \n",
    "        yEst = T(xTrain)\n",
    "        \n",
    "        # subtract the estimate based on current ensemble from the labels\n",
    "        yTrain = Y[sampIdx] - np.array(yEst).reshape([-1,1])\n",
    "        \n",
    "        # build a tree on the sampled data using residuals for labels\n",
    "        tree = DecisionTreeRegressor(max_depth=nDepth)\n",
    "        tree.fit(xTrain, yTrain)\n",
    "                \n",
    "        # add the new tree with a learning rate parameter (gamma)\n",
    "        T = wt_sum_fcn(T, tree.predict, gamma)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrees = 20  # try changing the number of trees being built\n",
    "nDepth = 1   # fairly deep for 100 data points\n",
    "gamma = 0.1\n",
    "bagFrac = 0.5   # Bag fraction - how many points in each of the random subsamples.  \n",
    "\n",
    "gbst = GradientBoosting(nTrees, nDepth, gamma, bagFrac, X, Y)\n",
    "\n",
    "result = gbst(X)\n",
    "\n",
    "plt.plot(X, result, 'r')\n",
    "plt.scatter(X,Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments and some things to try\n",
    "You may have noticed that the sampling machinery from bagging was left in the code for gradient boosting.  Friedman's first paper \"Greedy Function Approximation\" did not include sampling the input data.  But sampling and the basic mechanics of functional gradient descent are separate matters and Friedman's second paper \"Stochastic Gradient Boosting\" added that element.  Links to both these papers can be found below.  \n",
    "\n",
    "#### Some things to try\n",
    "- Try running some of the same experiments as you did with bagging. Change the tree depth, number of trees etc.  Also try the sine function for Y(X) and see how gradient boosting does with it.  \n",
    "\n",
    "Here are some things that will highlight an important difference between gradient boosting and methods that are primarily variance reduction techniques.  \n",
    "\n",
    "- Experiment with different tree depths and see how it affects the accuracy of the final model.  With bagging, you saw that using depth 1 trees resulted in a bias error that could not be overcome by adding more trees.  Does that happen with gradient boosting?  \n",
    "\n",
    "Since gradient boosting is constantly changing the labels to emphasize the portions of the X-space where it's making the most mistakes, it will eventually pay so much attention to the edges of the data that it will start putting split points for depth 1 trees at the extreme edges of the data.  That raises the question: \"Why would you ever use trees deeper than 1 with gradient boosting?\"  \n",
    "\n",
    "The reason for adding tree depth with gradient boosting is to cover problems where there is joint dependence on two or more variables and that dependence plays an important role in predicting the labels.  Modeling two-way or dependence requires that pairs of variables both affect some of the splits in a single tree.  That requires more than a single split in the trees.  Start with relatively shallow trees for gradient boosting.  After you've got that dialed in, then try more depth to see if you get an improvement.  \n",
    "\n",
    "I hope you like gradient boosted trees.  It has won more Kaggle competitions than any other algo.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "https://explained.ai/gradient-boosting/index.html - GBM explained (__READ THIS FIRST__)    \n",
    "https://statweb.stanford.edu/~jhf/ftp/trebst.pdf - Greedy Function Approximation - a Gradient Boosting Machine  \n",
    "https://statweb.stanford.edu/~jhf/ftp/stobst.pdf - Stochastic Gradient Boosting  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For __regression__ problems, the predicted response for an observation is the weighted average of the predictions using selected trees only. That is,\n",
    "\n",
    "$$\n",
    "\\hat{y}_{bag} = \\frac{1}{\\sum^T_{t=1}\\alpha_tI(t \\in S)}\\sum^T_{t=1}\\alpha_t\\hat{y}_tI(t \\in S)\n",
    "$$ \n",
    "\n",
    "\n",
    "* $\\hat{y}_i$ is the prediction from tree t in the ensemble.\n",
    "\n",
    "* $S$ is the set of indices of selected trees that comprise the prediction. $I(t \\in S)$ is $1$ if $t$ is in the set $S$, and $0$ otherwise.\n",
    "\n",
    "* $\\alpha_t$ is the weight of tree $t$.\n",
    "\n",
    "For __classification__ problems, the predicted class for an observation is the class that yields the largest weighted average of the class posterior probabilities (i.e., classification scores) computed using selected trees only. That is,\n",
    "\n",
    "* For each class $c \\in C$ and each tree $t = 1,...,T$, predict computes $\\hat{P}_t(c|x)$ which is the estimated posterior probability of class $c$ given observation $x$ using tree $t$. $C$ is the set of all distinct classes in the training data. \n",
    "\n",
    "* To make a prediction, compute the weighted average of the class posterior probabilities over the selected trees.\n",
    "\n",
    "$$\n",
    "\\hat{P}_{bag}(c|x) = \\frac{1}{\\sum^T_{t=1}\\alpha_tI(t \\in S)}\\sum^T_{t=1}\\alpha_t\\hat{P}_t(c|x)I(t \\in S)\n",
    "$$\n",
    "\n",
    "* The predicted class is the class that yields the largest weighted average.\n",
    "\n",
    "$$\n",
    "\\hat{y}_{bag} = argmax_{c \\in C} \\big\\{\\hat{P}_{bag}(c|x)\\big\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values when Applying Classification Models\n",
    "http://jmlr.csail.mit.edu/papers/volume8/saar-tsechansky07a/saar-tsechansky07a.pdf   \n",
    "It is important to distinguish two contexts: features may be missing at induction time, in the\n",
    "historical “training”data, or at prediction time, in to-be-predicted “test”cases. This paper compares\n",
    "techniques for handling missing values at prediction time.\n",
    "\n",
    "1. __Discard instances.__\n",
    "2. __Acquire missing values.__\n",
    "3. __Imputation.__\n",
    "4. __Reduced-feature Models:__ This can be accomplished by delaying\n",
    "model induction until a prediction is required, a strategy presented as “lazy” classificationtree induction by Friedman et al. (1996). Alternatively, for reduced-feature modeling one may\n",
    "store many models corresponding to various patterns of known and unknown test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
