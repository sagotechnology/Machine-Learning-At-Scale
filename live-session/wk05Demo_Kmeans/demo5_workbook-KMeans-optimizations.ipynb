{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 5 - K-Means Clustering in Spark\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2019`__\n",
    "\n",
    "\n",
    "By the end of this demo you should be able to:  \n",
    "* ... __implement__ K-Means in Spark.\n",
    "* ... __explain__ how the centroid initialization affects K-Means results & time to convergence.\n",
    "* ... __explain__ how different join operations are implemented in Spark\n",
    "* ... __explain__  the challenges of implementing the A Priori algorithm at Scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/root/demo4': File exists\n"
     ]
    }
   ],
   "source": [
    "# globals\n",
    "HOME_DIR = \"/media/notebooks\" # this is where docker mounts your repo, ADJUST AS NEEDED\n",
    "DEMO_DIR = HOME_DIR + \"/LiveSessionMaterials/wk05Demo_Kmeans\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Review: Kmeans\n",
    "\n",
    "K-Means clustering is an algorithm designed to group examples into K distinct groups based on their features. K-Means clustering can be used to categorize data when we do not have any concrete information about what those categories may look like. For example we may want to perform a market segmentation analysis on our customers but we don't actually know what the segments are.\n",
    "\n",
    "At a high level clustering is an attempt to group objects in such a way as to make those objects similar to other objects within the group and dissimilar to those outside of the group. In order to perform K-Means clustering we want to minimize the distance between all of the examples that are a part of a cluster and the center of that cluster, also called the _cluster centroid_. The clustering algorithm solves the following cost minimization:\n",
    "\n",
    "$$\n",
    "\\arg\\min_{c} \\sum_{i=1}^k\\sum_{{x}\\in c_i} \\left\\Vert {x}-\\mu_i \\right\\Vert^2\n",
    "$$\n",
    "\n",
    "It turns out that the above minimization problem is very difficult to solve (t falls into the class of problems known as [NP-Hard](https://en.wikipedia.org/wiki/NP-hardness)). The K-Means algorithm attempts to solve this clustering problem iteratively by repeating the 2-step process of marking each example as belonging to a single cluster by finding the closest cluster centroid\n",
    "and then adjusting each of the K cluster centroids so that each centroid is in the middle of all the examples that belong to that centroid.\n",
    "\n",
    "[Here](http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html) is a simple visualization of the process. There are a number of up front choices that must be made before the algorithm can be implemented:\n",
    "\n",
    "* __K__: the total number of centroids is selected using some apriori information about the desired outcome\n",
    "* __Distance Function__: typically the euclidian distance is used but this can be any function $d({x}, \\mu)$\n",
    "* __Convergence Criteria__: a rule used to determine when the iterative process can stop.\n",
    "\n",
    "The formal algorithm can then be defined as follows. First, decide the number of clusters $K$. Then:\n",
    "\n",
    "\n",
    "| Step | Description | Pseudocode|        \n",
    "| :---: |:---------------------------------------------: |:--------------- |\n",
    "|1 |Initialize the center ('centroid') of the clusters| ${\\mu}_i = $ random value $, i=1,...,k$|\n",
    "|2|Attribute the closest cluster to each data point|${c}_i = \\{j: d({x}_j, \\mu_i) \\le d({x}_j, \\mu_l),  l \\ne i, j=1,...,n\\}$|\n",
    "|3| Update centroid to the mean of points in that cluster|$\\mu_i = \\frac{1}{|c_i|}\\sum_{j\\in c_i} {x}_j,\\forall i$|\n",
    "|4|Repeat steps 2-3 until convergence criteria met||\n",
    "||Notation:|${|c|} = $ number of elements in  ${c}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Demo Data for K-means\n",
    "\n",
    "The Iris dataset was used in R.A. Fisher's classic 1936 paper, [The Use of Multiple Measurements in Taxonomic Problems](http://rcs.chemometrics.ru/Tutorials/classification/Fisher.pdf). It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n",
    "\n",
    "The columns in this dataset are:\n",
    "> `Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species`\n",
    "\n",
    "If you want to learn more about this dataset visit the [UCI Iris Data Set page](https://archive.ics.uci.edu/ml/datasets/iris). Use the cells below to download, preprocess and visualize this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data directory\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the iris dataset\n",
    "!curl -L -O https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few data cleaning steps... (remove blank last line, & separate features from labels)\n",
    "!head -n -1 iris.data > data/iris_full.csv\n",
    "!cut -d ',' -f 1-4 data/iris_full.csv > data/iris_features.csv\n",
    "!rm iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1,3.5,1.4,0.2,Iris-setosa\n",
      "4.9,3.0,1.4,0.2,Iris-setosa\n",
      "4.7,3.2,1.3,0.2,Iris-setosa\n",
      "4.6,3.1,1.5,0.2,Iris-setosa\n",
      "5.0,3.6,1.4,0.2,Iris-setosa\n",
      "5.4,3.9,1.7,0.4,Iris-setosa\n",
      "4.6,3.4,1.4,0.3,Iris-setosa\n",
      "5.0,3.4,1.5,0.2,Iris-setosa\n",
      "4.4,2.9,1.4,0.2,Iris-setosa\n",
      "4.9,3.1,1.5,0.1,Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "# take a look at the first few lines\n",
    "!head data/iris_full.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up constants for ease of directory use\n",
    "IRIS_DATA_WITH_LABELS = DEMO_DIR + '/data/iris_full.csv'\n",
    "IRIS_DATA = DEMO_DIR + '/data/iris_features.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.545187,5.924277,1.895756,0.790984\n",
      "2.737536,5.057883,1.430240,0.507271\n",
      "-0.119401,3.210891,1.724257,1.347952\n",
      "2.238231,3.674296,3.130839,-0.796941\n",
      "1.009556,4.362236,2.617186,-0.537835\n",
      "0.834667,4.475390,1.650515,1.138014\n",
      "2.588930,4.098863,1.034158,1.243179\n",
      "0.603402,4.933366,1.772641,0.999412\n",
      "1.279231,5.014413,1.492446,1.250908\n",
      "-0.236598,4.587557,1.773072,-0.260540\n"
     ]
    }
   ],
   "source": [
    "SKEW = DEMO_DIR + '/data/skewedClusters.txt'\n",
    "!head {SKEW}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     58882     63168\n",
      "     58959     63322\n",
      "     59034     60585\n",
      "     59145     61717\n",
      "     59164     59757\n",
      "     59168     61401\n",
      "     59250     61922\n",
      "     59318     62083\n",
      "     59458     61429\n",
      "     59464     60412\n"
     ]
    }
   ],
   "source": [
    "BIRCH = DEMO_DIR + '/data/birch2.txt'\n",
    "!head {BIRCH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Demo Data for Kmeans\n",
    "\n",
    "For purposes of visualization of this dataset we use a PCA decomposition to project the 4 dimensional iris dataset down to 2 dimensions. This will give us the ability to roughly visualize the effectiveness of our K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in labeled data\n",
    "mapping = {b'Iris-setosa': 0, \n",
    "           b'Iris-versicolor': 1, \n",
    "           b'Iris-virginica': 2}\n",
    "names = mapping.keys()\n",
    "data = np.loadtxt(IRIS_DATA_WITH_LABELS, delimiter=',', \n",
    "                  converters = {4: lambda s: mapping[s]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom plotting function\n",
    "from utils import plot_iris_data\n",
    "plot_iris_data(data[:,0:-1], data[:,-1], names, 'Iris Dataset (true labels)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "A simple (non-scalable) implementation in Python. For this example we will use the Iris dataset without labels and a K of 3 since we know ahead of time there are 3 different species labels (setosa, versicolor, and virginica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read in the data from the IRIS_DATA file\n",
    "samples = np.loadtxt(IRIS_DATA, \n",
    "                     delimiter=',')\n",
    "\n",
    "# define the number of clusters\n",
    "k = 3\n",
    "\n",
    "# define the distance function as the normalized distance\n",
    "def distance(a, b, ax=1):\n",
    "    return np.linalg.norm(a - b, axis=ax)\n",
    "\n",
    "def closestCluster(sample, centers):\n",
    "    distances = distance(sample, centers)\n",
    "    return np.argmin(distances)\n",
    "\n",
    "# define the convergence criteria\n",
    "convergenceCriteria = .001\n",
    "\n",
    "# (step1) set the k initial clusters by assigning a random point\n",
    "centroids = samples[np.random.choice(len(samples), size=k, replace=False)]\n",
    "\n",
    "while True:\n",
    "    # (step2) attribute the closest cluster to each point\n",
    "    clusters = [closestCluster(sample, centroids) \n",
    "                for sample \n",
    "                in samples]\n",
    "    \n",
    "    # (step3) set the position of each cluster to the mean of all data points belonging to that cluster\n",
    "    last_centroids = np.copy(centroids)\n",
    "    for i in range(k):\n",
    "        points = [samples[j] \n",
    "                  for j \n",
    "                  in range(len(samples)) if clusters[j] == i]\n",
    "        centroids[i] = np.mean(points, axis=0)\n",
    "    \n",
    "    # (step4) repeat steps 2-3 until convergence criteria met\n",
    "    error = distance(centroids, \n",
    "                     last_centroids, \n",
    "                     None)\n",
    "    if error < convergenceCriteria:\n",
    "        break\n",
    "\n",
    "print('-------Final Centroids-------')\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Examine the results by assigning each of the datapoints in the file to the nearest cluster and then plot all of the samples along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(map(lambda sample: closestCluster(sample, centroids), samples))\n",
    "plot_iris_data(np.array(samples), np.array(labels), ['cluster1', 'cluster2', 'cluster3'],\n",
    "                'Iris Dataset (Python K-Means clustering)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is boilerplate code required for a notebook in this enviroment to create a local spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"kmeans_demo_1\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Compare this Spark K-Means implementation to the original, non-scalable implementation in Python. Structurally they are quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 3.5, 1.4, 0.2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(IRIS_DATA) # 150 data points\n",
    "samples = lines.map(lambda line: np.array([float(x) for x in line.split(',')]))\n",
    "samples.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58882., 63168.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(BIRCH) # 100,000 data points\n",
    "samples = lines.map(lambda line: np.array([float(x) for x in line.split()]))\n",
    "samples.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.545187, 5.924277, 1.895756, 0.790984])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(SKEW) # 100,000 data points\n",
    "samples = lines.map(lambda line: np.array([float(x) for x in line.split(',')]))\n",
    "samples.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of clusters\n",
    "k = 3\n",
    "\n",
    "# define the distance function as the normalized distance\n",
    "def distance(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "def closestCluster(p, centers):\n",
    "    bestIndex = 0\n",
    "    closest = float(\"+inf\")\n",
    "    for i in range(len(centers)):\n",
    "        tempDist = distance(p, centers[i])\n",
    "        if tempDist < closest:\n",
    "            closest = tempDist\n",
    "            bestIndex = i\n",
    "    return bestIndex\n",
    "\n",
    "def addFunc(a,b):\n",
    "    return a+b\n",
    "# define the convergence criteria\n",
    "convergenceCriteria = .001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-newCentroids-\n",
      "[(2, array([528587.87246026,  41758.74803936])), (0, array([842645.5552492 ,  37698.21845175])), (1, array([209970.72125405,  51278.50064763]))]\n",
      "-------Final Centroids-------\n",
      "[[842645.5552492   37698.21845175]\n",
      " [209970.72125405  51278.50064763]\n",
      " [528587.87246026  41758.74803936]]\n",
      "17\n",
      "CPU times: user 269 ms, sys: 60.2 ms, total: 329 ms\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# (step1) set the k initial clusters by assigning a random point\n",
    "centroids = samples.takeSample(False, k, 1)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    iteration+=1\n",
    "    # (step2) attribute the closest cluster to each point\n",
    "    # returns tuple (clusterId, (dataPoint, 1))\n",
    "    clusters = samples.map(lambda p: (closestCluster(p, centroids), (p, 1)))\n",
    "    \n",
    "    # reduceByKey returns tuple (clusterId, (sumOfPoints, sumOfCounts))\n",
    "    # map returns tuples (clusterId, avgOfPoints) for each cluster\n",
    "    # p1_c1 => point1_count1\n",
    "    newCentroids = clusters.reduceByKey(lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1])) \\\n",
    "                        .map(lambda st: (st[0], st[1][0] / st[1][1])) \\\n",
    "                        .collect()\n",
    "    \n",
    "    error = sum(np.sum(distance(centroids[iK], p)) for (iK, p) in newCentroids)\n",
    "        \n",
    "    # (step3) set the position of each cluster to the mean of all data points belonging to that cluster\n",
    "    for (iK, p) in newCentroids:\n",
    "        centroids[iK] = p\n",
    "        \n",
    "    # (step4) repeat steps 2-3 until convergence criteria met\n",
    "    if error < convergenceCriteria:\n",
    "        break\n",
    "print(\"-newCentroids-\")\n",
    "print(newCentroids)\n",
    "print('-------Final Centroids-------')\n",
    "print(np.array(centroids))\n",
    "print(iteration)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BIRCH data\n",
    "\n",
    "Per Iteration:\n",
    "Total Time Across All Tasks: 3 s\n",
    "Locality Level Summary: Process local: 2\n",
    "Input Size / Records: 2.1 MB / 100000\n",
    "Shuffle Write: 965.0 B / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 282 ms, sys: 70.6 ms, total: 353 ms\n",
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# (step1) set the k initial clusters by assigning a random point\n",
    "centroids = samples.takeSample(False, k, 1)\n",
    "iteration=0\n",
    "while True:\n",
    "    iteration+=1\n",
    "    # (step2) attribute the closest cluster to each point\n",
    "    # returns tuple (clusterId, dataPoint, 1)\n",
    "    clusters = samples.map(lambda p: (closestCluster(p, centroids), p))\n",
    "    \n",
    "    # groupByKey returns tuple (clusterId, (iterator))\n",
    "    # map returns tuples (clusterId, avgOfPoints) for each cluster\n",
    "    newCentroids = clusters.groupByKey()\\\n",
    "                           .map(lambda x: (x[0], rd(addFunc, x[1])/len(x[1])))\\\n",
    "                           .collect()\n",
    "    \n",
    "    error = sum(np.sum(distance(centroids[iK], p)) for (iK, p) in newCentroids)\n",
    "        \n",
    "    # (step3) set the position of each cluster to the mean of all data points belonging to that cluster\n",
    "    for (iK, p) in newCentroids:\n",
    "        centroids[iK] = p\n",
    "        \n",
    "    # (step4) repeat steps 2-3 until convergence criteria met\n",
    "    if error < convergenceCriteria:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33970"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets = clusters.groupByKey().glom().collect()\n",
    "c=0\n",
    "len(sets[1][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-newCentroids-\n",
      "[(2, array([528587.87246026,  41758.74803936])), (0, array([842645.5552492 ,  37698.21845175])), (1, array([209970.72125405,  51278.50064763]))]\n",
      "-------Final Centroids-------\n",
      "[[842645.5552492   37698.21845175]\n",
      " [209970.72125405  51278.50064763]\n",
      " [528587.87246026  41758.74803936]]\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(\"-newCentroids-\")\n",
    "print(newCentroids)\n",
    "print('-------Final Centroids-------')\n",
    "print(np.array(centroids))\n",
    "print(iteration)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BIRCH data\n",
    "\n",
    "The groupBy implementation/Iteration:\n",
    "Total Time Across All Tasks: 3 s\n",
    "Locality Level Summary: Process local: 2\n",
    "Input Size / Records: 2.1 MB / 100000\n",
    "Shuffle Write: 3.6 MB / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregateByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PipelinedRDD' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'PipelinedRDD' object is not iterable"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def addPartials(a,b):\n",
    "    return (a[0]+b[0], a[1]+b[1])\n",
    "\n",
    "def takeAvg(a,b):\n",
    "    return (a[0]+b[0])/(a[1]+b[1])\n",
    "\n",
    "# (step1) set the k initial clusters by assigning a random point\n",
    "centroids = samples.takeSample(False, k, 1)\n",
    "iteration=0\n",
    "while True:\n",
    "    iteration+=1\n",
    "    # (step2) attribute the closest cluster to each point\n",
    "    # returns tuple (clusterId, dataPoint, 1)\n",
    "    clusters = samples.map(lambda p: (closestCluster(p, centroids), (p, 1)))\n",
    "    \n",
    "\n",
    "    newCentroids = clusters.aggregateByKey(0,addPartials,takeAvg)\n",
    "    \n",
    "    error = sum(np.sum(distance(centroids[iK], p)) for (iK, p) in newCentroids)\n",
    "        \n",
    "    # (step3) set the position of each cluster to the mean of all data points belonging to that cluster\n",
    "    for (iK, p) in newCentroids:\n",
    "        centroids[iK] = p\n",
    "        \n",
    "    # (step4) repeat steps 2-3 until convergence criteria met\n",
    "    if error < convergenceCriteria:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.12031365  5.75806083]\n",
      " [ 1.7373078   4.42546234]\n",
      " [ 2.36833522  0.04356792]\n",
      " [ 0.87305123  4.71438583]\n",
      " [-0.66246781  2.17571724]\n",
      " [ 0.74285061  1.46351659]\n",
      " [-4.07989383  3.57150086]\n",
      " [ 3.54934659  0.6925054 ]\n",
      " [ 2.49913075  1.23133799]\n",
      " [ 1.9263585   4.15243012]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=10, centers=3, n_features=2,random_state=0)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10007, 4)\n"
     ]
    }
   ],
   "source": [
    "X, y = make_blobs(n_samples=[10000, 3, 4], centers=None, n_features=4,random_state=0)\n",
    "print(X.shape)\n",
    "np.savetxt('skewedClusters.txt', X, delimiter=',',fmt='%8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.20.2.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Examine the results by assigning each of the datapoints in the file to the nearest cluster and then plot all of the samples along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.loadtxt(IRIS_DATA, \n",
    "                     delimiter=',')\n",
    "labels = list(map(lambda sample: closestCluster(sample, centroids), \n",
    "             samples))\n",
    "plot_iris_data(np.array(samples), \n",
    "     np.array(labels), \n",
    "     ['cluster1', 'cluster2', 'cluster3'],\n",
    "     'Iris Dataset (Spark K-Means clustering)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion\n",
    "\n",
    "How does changing the centroid initialization affect the result?\n",
    "How does changing the convergence criteria affect the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See also:\n",
    "* KMeans++\n",
    "* KMeans||\n",
    "* Canopy clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulators\n",
    "Definitive Guide book, pg. 241"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulators are Spark's equivalent of Hadoop counters. Like broadcast variables they represent shared information across the nodes in your cluster, but unlike broadcast variables accumulators are _write-only_ ... in other words you can only access their values in the driver program and not on your executors (where transformations are applied). As convenient as this sounds, there are a few common pitfalls to avoid. Let's take a look.\n",
    "\n",
    "Run the following cell to create a sample data file representing a list of `studentID, courseID, final_grade`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/grades.csv\n",
    "10001,101,98\n",
    "10001,102,87\n",
    "10002,101,75\n",
    "10002,102,55\n",
    "10002,103,80\n",
    "10003,102,45\n",
    "10003,103,75\n",
    "10004,101,90\n",
    "10005,101,85\n",
    "10005,103,60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to compute the average grade by course and student while also tracking the number of failing grades awarded. We might try something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to increment the accumulator as we read in the data\n",
    "def parse_grades(line, accumulator):\n",
    "    \"\"\"Helper function to parse input & track failing grades.\"\"\"\n",
    "    student,course,grade = line.split(',')\n",
    "    grade = int(grade)\n",
    "    if grade < 65:\n",
    "        accumulator.add(1)\n",
    "    return(student,course, grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an accumulator to track failing grades\n",
    "nFailing = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute averages in spark\n",
    "\n",
    "gradesRDD = sc.textFile('data/grades.csv')\\\n",
    "              .map(lambda x: parse_grades(x, nFailing))\n",
    "\n",
    "studentAvgs = gradesRDD.map(lambda x: (x[0], (x[2], 1)))\\\n",
    "                       .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n",
    "                       .mapValues(lambda x: x[0]/x[1])\n",
    "\n",
    "courseAvgs = gradesRDD.map(lambda x: (x[1], (x[2], 1)))\\\n",
    "                      .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n",
    "                      .mapValues(lambda x: x[0]/x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look\n",
    "print(\"===== average by student =====\")\n",
    "print(studentAvgs.collect())\n",
    "print(\"===== average by course =====\")\n",
    "print(courseAvgs.collect())\n",
    "print(\"===== number of failing grades awarded =====\")\n",
    "print(nFailing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __DISCUSSION QUESTIONS:__\n",
    "* What is wrong with the results? (__`HINT:`__ _how many failing grades are there really?_)\n",
    "* Why might this be happening? (__`HINT:`__ _How many actions are there in this code? Which parts of the DAG are recomputed for each of these actions?_)\n",
    "* What one line could we add to the code to fix this problem?\n",
    "  * What could go wrong with our \"fix\"?\n",
    "* How could we have designed our parser differently to avoid this problem in the first place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Accumulators\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While SparkContext supports accumulators for primitive data types like int and float, users can also define accumulators for custom types by providing a custom AccumulatorParam object. \n",
    "\n",
    "We may want to utilize custom accumulators later in the course when we implement PageRank, or Shortest Path (graph) algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "# Spark only implements Accumulator parameter for numeric types.\n",
    "# This class extends Accumulator support to the string type.\n",
    "class StringAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return value\n",
    "    def addInPlace(self, val1, val2):\n",
    "        return val1 +\" -> \"+ val2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename=\"aggregations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to get a list of letter grades that each student recieved as well as their average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLetterGrade(x):\n",
    "    if x > 92.0:\n",
    "        return \"A\"\n",
    "    elif x > 82.0:\n",
    "        return \"B\"\n",
    "    elif x > 72.0:\n",
    "        return \"C\"\n",
    "    elif x > 65.0:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"F\"\n",
    "\n",
    "def getCounts(a,b):\n",
    "    return (a[0] + b[0], a[1] + b[1], toLetterGrade(a[0])+toLetterGrade(b[0]))\n",
    "    \n",
    "studentAvgs = gradesRDD.map(lambda x: (x[0], (x[2], 1)))\\\n",
    "                       .reduceByKey(getCounts)\\\n",
    "                       .mapValues(lambda x: ((x[0]/x[1]),x[2]))\\\n",
    "                       .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename=\"Bob-Ross-3.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradesRDD.map(lambda x: (x[0], (x[2], 1))).collect()\n",
    "gradesRDD.map(lambda x: (x[0], (x[2], 1))).reduceByKey(getCounts).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foldByKey allows us to specify a zero value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradesRDD.map(lambda x: (x[0], (x[2], 1))).foldByKey((0,0,\"\"),getCounts).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we solve this problem using a combineByKey which provides more granular control over the parameters\n",
    "https://backtobazics.com/big-data/apache-spark-combinebykey-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCombiner(a):\n",
    "    return a\n",
    "\n",
    "def mergeValues(a,b):\n",
    "    return (a[0] + b[0], a[1] + b[1], toLetterGrade(a[0])+toLetterGrade(b[0]));\n",
    "\n",
    "def mergeCombiners(a,b):\n",
    "    return (a[0] + b[0], a[1] + b[1], toLetterGrade(a[0])+toLetterGrade(b[0]))\n",
    "\n",
    "studentAvgs = gradesRDD.map(lambda x: (x[0], (x[2], 1)))\\\n",
    "                       .combineByKey(createCombiner,mergeValues,mergeCombiners)\\\n",
    "                       .mapValues(lambda x: ((x[0]/x[1]),x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradesRDD.map(lambda x: (x[0], (x[2], 1))).combineByKey(createCombiner,mergeValues,mergeCombiners).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggragateByKey requires a null and start value as well as two different functions. One to aggregate within partitions, and one to aggregate across partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqOp(a,b):\n",
    "    return(a[0] + b[0], a[1] + b[1], a[2]+toLetterGrade(b[2]))\n",
    "\n",
    "def combOp(a,b):\n",
    "    return (a+b);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letterAccum = sc.accumulator(\"===\", StringAccumulatorParam())\n",
    "\n",
    "gradesRDD.foreach(lambda x: letterAccum.add(toLetterGrade(x[2])))\n",
    "\n",
    "gradesRDD.map(lambda x: (x[0], (x[2], 1, x[2])))\\\n",
    "         .aggregateByKey((0,0,\"\"),seqOp,combOp)\\\n",
    "         .mapValues(lambda x: ((x[0]/x[1]),x[2]))\\\n",
    "         .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (letterAccum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* join       \n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.join     \n",
    "* leftOuterJoin   \n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.leftOuterJoin    \n",
    "* rightOuterJoin   \n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.rightOuterJoin    \n",
    "* fullOuterJoin   \n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.fullOuterJoin   \n",
    "* cartesian   \n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD.cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
    "sorted(x.fullOuterJoin(y).collect())\n",
    "#[('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(x.rightOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(x.leftOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load some data for the code examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    "  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")])\\\n",
    "  .toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run as is\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run as is\n",
    "wrongJoinExpression = person[\"name\"] == graduateProgram[\"school\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run as is\n",
    "person.join(graduateProgram, joinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(graduateProgram, wrongJoinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark perfoms an \"inner\" join by default. But we can specify this explicitly.\n",
    "# Try different join types.\n",
    "joinType = \"outer\"\n",
    "joinType = \"left_outer\"\n",
    "joinType = \"right_outer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which keys do outer joins evaluate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A departure from traditional joins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    "    (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))\n",
    "gradProgram2.createOrReplaceTempView(\"gradProgram2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think of left semi joins as filters on a DataFrame, as opposed to the function of a conventional join\n",
    "joinType = \"left_semi\"\n",
    "gradProgram2.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradProgram2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"left_anti\"\n",
    "gradProgram2.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Joins\n",
    "\n",
    "__DANGER__: Natural joins make implicit guesses at the columns on which you would like to join. Why is this bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross (Cartesian) Joins\n",
    "Or, Cartesian products. Cross joinsare inner joins that do not specify a predicate. Cross joins will join every single row in the left DataFrame with every single row in the right DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.crossJoin(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DANGER__: How many rows would we end up with from a cross join if each table had 1000 rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Priori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(see WK5 slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------- END NOTEBOOK --------------------\n",
    "## Future work: Clustering for anomaly detection   \n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.323.6870&rep=rep1&type=pdf   \n",
    "https://proquest-safaribooksonline-com.libproxy.berkeley.edu/book/databases/9781491972946   \n",
    "https://towardsdatascience.com/best-clustering-algorithms-for-anomaly-detection-d5b7412537c8   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDD Cup 1999 Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each connection, the data set contains information like the number of bytes sent, login attempts, TCP errors, and so on. Each connection is one line of CSV-formatted data set, containing 38 features, like this:\n",
    "\n",
    "```\n",
    "0,tcp,http,SF,215,45076,\n",
    "0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,\n",
    "0.00,0.00,0.00,0.00,1.00,0.00,0.00,0,0,0.00,\n",
    "0.00,0.00,0.00,0.00,0.00,0.00,0.00,normal.\n",
    "```\n",
    "\n",
    "This connection, for example, was a TCP connection to an HTTP service—215 bytes were sent and 45,706 bytes were received. The user was logged in, and so on. Many features are counts, like num_file_creations in the 17th column.\n",
    "\n",
    "Many features take on the value 0 or 1, indicating the presence or absence of a behavior, like su_attempted in the 15th column. They look like the one-hot encoded categorical features from Chapter 4, but are not grouped and related in the same way. Each is like a yes/no feature, and is therefore arguably a categorical feature. It is not always valid to translate categorical features as numbers and treat them as if they had an ordering. However, in the special case of a binary categorical feature, in most machine learning algorithms, mapping these to a numeric feature taking on values 0 and 1 will work well.\n",
    "\n",
    "The rest are ratios like dst_host_srv_rerror_rate in the next-to-last column, and take on values from 0.0 to 1.0, inclusive.\n",
    "\n",
    "Interestingly, a label is given in the last field. Most connections are labeled normal., but some have been identified as examples of various types of network attacks. These would be useful in learning to distinguish a known attack from a normal connection, but the problem here is anomaly detection and finding potentially new and unknown attacks. This label will be mostly set aside for our purposes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A First Take on Clustering\n",
    "Unzip the kddcup.data.gz data file and copy it into HDFS. This example, like others, will assume the file is available at /user/ds/kddcup.data. Open the spark-shell, and load the CSV data as a data frame. It’s a CSV file again, but without header information. It’s necessary to supply column names as given in the accompanying kddcup.names file.\n",
    "\n",
    "val dataWithoutHeader = spark.read.\n",
    "  option(\"inferSchema\", true).\n",
    "  option(\"header\", false).\n",
    "  csv(\"hdfs:///user/ds/kddcup.data\")\n",
    "\n",
    "val data = dataWithoutHeader.toDF(\n",
    "  \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "  \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "  \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "  \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "  \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "  \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "  \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "  \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "  \"dst_host_count\", \"dst_host_srv_count\",\n",
    "  \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "  \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "  \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "  \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "  \"label\")\n",
    "Begin by exploring the data set. What labels are present in the data, and how many are there of each? The following code simply counts by label and prints the results in descending order by count.\n",
    "\n",
    "data.select(\"label\").groupBy(\"label\").count().orderBy($\"count\".desc).show(25)\n",
    "\n",
    "...\n",
    "+----------------+-------+\n",
    "|           label|  count|\n",
    "+----------------+-------+\n",
    "|          smurf.|2807886|\n",
    "|        neptune.|1072017|\n",
    "|         normal.| 972781|\n",
    "|          satan.|  15892|\n",
    "...\n",
    "|            phf.|      4|\n",
    "|           perl.|      3|\n",
    "|            spy.|      2|\n",
    "+----------------+-------+\n",
    "There are 23 distinct labels, and the most frequent are smurf. and neptune. attacks.\n",
    "\n",
    "Note that the data contains nonnumeric features. For example, the second column may be tcp, udp, or icmp, but K-means clustering requires numeric features. The final label column is also nonnumeric. To begin, these will simply be ignored.\n",
    "\n",
    "Aside from this, creating a K-means clustering of the data follows the same pattern as was seen in Chapter 4. A VectorAssembler creates a feature vector, a KMeans implementation creates a model from the feature vectors, and a Pipeline stitches it all together. From the resulting model, it’s possible to extract and examine the cluster centers.\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val numericOnly = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n",
    "\n",
    "val assembler = new VectorAssembler().\n",
    "  setInputCols(numericOnly.columns.filter(_ != \"label\")).\n",
    "  setOutputCol(\"featureVector\")\n",
    "\n",
    "val kmeans = new KMeans().\n",
    "  setPredictionCol(\"cluster\").\n",
    "  setFeaturesCol(\"featureVector\")\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, kmeans))\n",
    "val pipelineModel = pipeline.fit(numericOnly)\n",
    "val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "\n",
    "kmeansModel.clusterCenters.foreach(println)\n",
    "\n",
    "...\n",
    "[48.34019491959669,1834.6215497618625,826.2031900016945,793027561892E-4,...\n",
    "[10999.0,0.0,1.309937401E9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,...\n",
    "It’s not easy to interpret the numbers intuitively, but each of these represents the center (also known as centroid) of one of the clusters that the model produced. The values are the coordinates of the centroid in terms of each of the numeric input features.\n",
    "\n",
    "Two vectors are printed, meaning K-means was fitting k=2 clusters to the data. For a complex data set that is known to exhibit at least 23 distinct types of connections, this is almost certainly not enough to accurately model the distinct groupings within the data.\n",
    "\n",
    "This is a good opportunity to use the given labels to get an intuitive sense of what went into these two clusters by counting the labels within each cluster.\n",
    "\n",
    "val withCluster = pipelineModel.transform(numericOnly)\n",
    "\n",
    "withCluster.select(\"cluster\", \"label\").\n",
    "  groupBy(\"cluster\", \"label\").count().\n",
    "  orderBy($\"cluster\", $\"count\".desc).\n",
    "  show(25)\n",
    "\n",
    "...\n",
    "+-------+----------------+-------+\n",
    "|cluster|           label|  count|\n",
    "+-------+----------------+-------+\n",
    "|      0|          smurf.|2807886|\n",
    "|      0|        neptune.|1072017|\n",
    "|      0|         normal.| 972781|\n",
    "|      0|          satan.|  15892|\n",
    "|      0|        ipsweep.|  12481|\n",
    "...\n",
    "|      0|            phf.|      4|\n",
    "|      0|           perl.|      3|\n",
    "|      0|            spy.|      2|\n",
    "|      1|      portsweep.|      1|\n",
    "+-------+----------------+-------+\n",
    "The result shows that the clustering was not at all helpful. Only one data point ended up in cluster 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "462px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "567px",
    "left": "0px",
    "right": "707.4456787109375px",
    "top": "105px",
    "width": "243px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
