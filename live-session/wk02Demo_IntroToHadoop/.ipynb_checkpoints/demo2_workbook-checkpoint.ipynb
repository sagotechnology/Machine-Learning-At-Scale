{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wk2 Demo - Intro to Hadoop Streaming\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information`__\n",
    "\n",
    "Last week you implemented your first MapReduce Algorithm using a bash script framework. We saw that adding a sorting component to our framework allowed us to write a more efficient reducer script and perform word counting in parallel. In this notebook, we'll introduce a new framework: Hadoop Streaming. Like before, you'll write mapper and reducer scripts in python then pass them to the framework which will stream over your input files, split them into chunks and sort to your specification. Although Hadoop Streaming is rarely used in production anymore it is the precursor to systems like Spark and as such is a useful way to illustrate key concepts in parallel computation. By the end of this live session you should be able to:\n",
    "* __... describe__ the main components and default behavior of the Hadoop Streaming framework.\n",
    "* __... write__ a Hadoop MapReduce job from scratch.\n",
    "* __... access__ the Hadoop Streaming UI and use it in debugging your jobs.\n",
    "* __... design__ Hadoop MapReduce implementations for simple tasks like counting and ordering.\n",
    "* __... explain__ why sorting with multiple reducers requires some extra work (as opposed to sorting with a single reducer).\n",
    "\n",
    "**Note**: Hadoop Streaming syntax is very particular. Make sure to test your python scripts before passing them to the Hadoop job and pay careful attention to the order in which Hadoop job parameters are specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For convenience, let's set a few global variables for paths you'll use frequently. __`NOTE:`__ _you may need to modify the jar file and HDFS (or local home) directory paths to match your environment. The paths below should work on the course Docker image. Refer to_ [this debugging FAQ](https://github.com/UCB-w261/main/blob/master/HelpfulResources/Hadoop/debugging-hadoop.md) _if you are unsure of the correct paths or encounter errors._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop 2.6.0-cdh5.16.2\n",
      "Subversion http://github.com/cloudera/hadoop -r 4f94d60caa4cbb9af0709a2fd96dc3861af9cf20\n",
      "Compiled by jenkins on 2019-06-03T10:43Z\n",
      "Compiled with protoc 2.5.0\n",
      "From source with checksum 79b9b24a29c6358b53597c3b49575e37\n",
      "This command was run using /usr/lib/hadoop/hadoop-common-2.6.0-cdh5.16.2.jar\n"
     ]
    }
   ],
   "source": [
    "!hadoop version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Streaming Docs\n",
    "https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/HadoopStreaming.html   \n",
    "Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to java archive files for the hadoop streaming application on your machine\n",
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# hdfs directory where  we'll store files for this assignment\n",
    "HDFS_DIR = \"/user/root/demo2\"\n",
    "!hdfs dfs -mkdir {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local directory where you've cloned the course repo \n",
    "# in docker this is the path where your clone is mounted -- ADJUST AS NEEDED\n",
    "HOME_DIR = \"/media/notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PWD = !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/notebooks/LiveSessionMaterials/wk02Demo_IntroToHadoop']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/notebooks/LiveSessionMaterials/wk02Demo_IntroToHadoop'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store notebook environment path\n",
    "from os import environ\n",
    "PATH  = environ['PATH']\n",
    "# NOTE: we can pass this variable to our Hadoop Jobs using -cmdenv PATH={PATH}\n",
    "# This will ensure that, among other things, Hadoop uses the right python version.\n",
    "# You should not *need* this until the very last question in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "In this notebook, we'll continue working with the  _Alice in Wonderland_ text file from HW1 and the test file we created for debugging. Run the following cell to confirm that you have access to these files and save their location to a global variable to use in your Hadoop Streaming jobs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data subfolder - RUN THIS CELL AS IS (if Option 1 failed)\n",
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  170k  100  170k    0     0   479k      0 --:--:-- --:--:-- --:--:--  479k\n"
     ]
    }
   ],
   "source": [
    "# (Re)Download Alice Full text from Project Gutenberg - RUN THIS CELL AS IS (if Option 1 failed)\n",
    "# NOTE: feel free to replace 'curl' with 'wget' or equivalent command of your choice.\n",
    "!curl \"http://www.gutenberg.org/files/11/11-0.txt\" -o data/alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/alice_test.txt\n",
    "This is a small test file. This file is for a test.\n",
    "This small test file has two small lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the paths - RUN THIS CELL AS IS (if Option 1 failed)\n",
    "ALICE_TXT = PWD + \"/data/alice.txt\"\n",
    "TEST_TXT = PWD + \"/data/alice_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### alice.txt #########\n",
      "﻿The Project Gutenberg EBook of Alice’s Adventures in Wonderland, by Lewis Carroll\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "######### alice_test.txt #########\n",
      "This is a small test file. This file is for a test.\n",
      "This small test file has two small lines.\n"
     ]
    }
   ],
   "source": [
    "# confirm the files are there - RUN THIS CELL AS IS\n",
    "!echo \"######### alice.txt #########\"\n",
    "!head -n 6 {ALICE_TXT}\n",
    "!echo \"######### alice_test.txt #########\"\n",
    "!cat {TEST_TXT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Overview: MapReduce Programming Paradigm\n",
    "The week 2 reading from _Data Intensive Text Processing with Map Reduce_ by Lin and Dyer gave a high level overview of the key issues faced by parallel computation frameworks. It also introduced the Hadoop MapReduce framework. Lets start by briefly reviewing some of the key concepts from this week's async:\n",
    "\n",
    "> __DISCUSSION QUESTIONS:__  \n",
    "* What is MapReduce? How does it differ from Hadoop?  \n",
    "* What are the main ideas of the functional programming paradigm and how does MapReduce exemplify these ideas?\n",
    "* What is the basic data structure used in Hadoop MapReduce?\n",
    "* What does 'data/code co-location' mean? How does this principle contribute to the efficiency of a distributed computation?\n",
    "* What is a race condition in the context of parallel computation? Give an example.\n",
    "* What kind of _synchronization_ does Hadoop MapReduce perform by default? \n",
    "* What aspect of the synchronization process is computationally costly?\n",
    "* What does Hadoop sort by? What is Hadoop's default sort order? \n",
    "* Throughout this course we'll emphasize the goal of writing 'stateless' implementations? What does that mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview: Hadoop Streaming Syntax\n",
    "A basic Hadoop MapReduce job consists of three components: a mapper script, a reducer script and a  line of code in which you pass these scripts to the Hadoop streaming application/framework. The mapper and reducer can be any executable that will read from `stdin` and write to `stdout`, including a bash executable like`/bin/cat` which simply passes the lines of your file unchanged, or python scripts like the onese you wrote in HW1. \n",
    "\n",
    "To run your hadoop streaming job, you'll need an HDFS filepath for the input data. We can use the following line of code to load a local file into HDFS:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/demo2/alice_test.txt\n",
      "Deleted /user/root/demo2/alice.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/alice_test.txt\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the alice test file into HDFS - RUN THIS CELL AS IS\n",
    "!hdfs dfs -copyFromLocal {TEST_TXT} {ALICE_TXT} {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup     174484 2020-09-05 17:44 /user/root/demo2/alice.txt\n",
      "-rw-r--r--   1 root supergroup         94 2020-09-05 17:44 /user/root/demo2/alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`TIP:`__ _Recall that we set the global variable_ `HDFS_DIR` _above to point to a directory called_ `demo2` _inside_ `/user/root/`, _you confirm that we've successfully loaded the data into HDFS using the following line. You can learn more about this command and others like it by reading the [Hadoop File System Shell Guide](https://hadoop.apache.org/docs/r2.7.5/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 root supergroup         94 2020-05-15 13:30 /user/root/demo2/alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "# confirm the data was loaded - RUN THIS CELL AS IS\n",
    "!hdfs dfs -ls {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, with that little bit of preparation, we can now run the most basic Hadoop Streaming Job.\n",
    "\n",
    ">__`DISCUSSION QUESTIONS`__ (_before you run the next cell ..._)   \n",
    "* Read the 6 lines below, what do each of them tell the framework to do?\n",
    "* What to the forward slashes indicate?\n",
    "* What do you expect to happen when we run this cell? (what should the output be? will it be printed to the console?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob6620184534745077098.jar tmpDir=null\n",
      "20/09/05 17:48:48 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/09/05 17:48:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/09/05 17:48:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "20/09/05 17:48:51 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/09/05 17:48:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1599063987985_0001\n",
      "20/09/05 17:48:53 INFO impl.YarnClientImpl: Submitted application application_1599063987985_0001\n",
      "20/09/05 17:48:53 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1599063987985_0001/\n",
      "20/09/05 17:48:53 INFO mapreduce.Job: Running job: job_1599063987985_0001\n",
      "20/09/05 17:49:14 INFO mapreduce.Job: Job job_1599063987985_0001 running in uber mode : false\n",
      "20/09/05 17:49:14 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/09/05 17:49:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/09/05 17:49:37 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/09/05 17:49:37 INFO mapreduce.Job: Job job_1599063987985_0001 completed successfully\n",
      "20/09/05 17:49:38 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=106\n",
      "\t\tFILE: Number of bytes written=443755\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=369\n",
      "\t\tHDFS: Number of bytes written=96\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19364\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7862\n",
      "\t\tTotal time spent by all map tasks (ms)=19364\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7862\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=19364\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7862\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19828736\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8050688\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2\n",
      "\t\tMap output records=2\n",
      "\t\tMap output bytes=96\n",
      "\t\tMap output materialized bytes=112\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=112\n",
      "\t\tReduce input records=2\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=4\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=283\n",
      "\t\tCPU time spent (ms)=5420\n",
      "\t\tPhysical memory (bytes) snapshot=783003648\n",
      "\t\tVirtual memory (bytes) snapshot=4109844480\n",
      "\t\tTotal committed heap usage (bytes)=819462144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=141\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=96\n",
      "20/09/05 17:49:38 INFO streaming.StreamJob: Output directory: /user/root/demo2/test-output\n"
     ]
    }
   ],
   "source": [
    "# first hadoop streaming job - RUN THIS CELL AS IS\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/alice_test.txt \\\n",
    "  -output {HDFS_DIR}/test-output \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that running the cell above doesn't actually show us the results. That's because the results get written directly to the output directory in HDFS. You can view the results using an `hdfs` command. (__`Note:`__ `test-output` _is an HDFS directory that was created by the 5th line in the last cell, we could have named it anything we wanted_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2020-05-15 13:31 /user/root/demo2/test-output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         96 2020-05-15 13:31 /user/root/demo2/test-output/part-00000\n"
     ]
    }
   ],
   "source": [
    "# view the contents of the result directory - RUN THIS CELL AS IS\n",
    "!hdfs dfs -ls {HDFS_DIR}/test-output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a small test file. This file is for a test.\t\n",
      "This small test file has two small lines.\t\n"
     ]
    }
   ],
   "source": [
    "# view the results themselves - RUN THIS CELL AS IS\n",
    "!hdfs dfs -cat {HDFS_DIR}/test-output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__`DISCUSSION QUESTIONS`__ (_after running the Hadoop Job._)   \n",
    "* Do the results match your expectations?\n",
    "* Scan the Hadoop Job logging, what information stands out to you?\n",
    "* When would it be a bad idea to print the full results of a job to the console? what could we do instead?\n",
    "* What goes wrong if you try re-running the Hadoop Streaming job? Discuss two potential solutions to this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout 1: WordCount in Hadoop MapReduce\n",
    "For most of the Hadoop jobs you write this week, you will use python scripts similar to those you wrote in HW1 to serve as your mapper and reducer. Since the Hadoop Streaming framework implements the principle of data/code co-location, we'll need to provide the paths to these python scripts so that Hadoop can ship them to the nodes where the code will get run. We do this by adding an additional flag to the Hadoop streaming command: the `-files` parameter. This will be the first of a number of additional flags that you can added to your Hadoop Streaming jobs to specify how the framework should handle your data. __`TIP:`__ _Hadoop can be very particular about the order in which you specify optional configuration fields. As a good debugging practice we recommend that you always maintain working code by starting with a basic job like the one we provided above and then testing the command as you add or modify parameters one by one_.\n",
    "\n",
    "For your first breakout activity we've provided an example of a Hadoop MapReduce job that performs word counting on an input file of your choice. The mapper and reducer are python scripts provided at __`WordCount/mapper.py`__ and __`WordCount/reducer.py`__. The Hadoop Streaming command is in a cell below. As you read through the example and go on to write your own Hadoop MapReduce jobs you may want to refer to Michael Noll's [blogpost on writing an MapReduce job](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/) and/or the [Hadoop Streaming documentation](https://hadoop.apache.org/docs/r2.7.5/hadoop-streaming/HadoopStreaming.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 1 Tasks:\n",
    "* __a) read scripts & docstrings:__ Read through **`WordCount/mapper.py`** and **`WordCount/reducer.py`** scripts and pay attention to the docstrings. Note that they (briefly) explain what the script does and the expected input/output record formats. [`HINT`: _docstrings are a way to record information to help your reader (future-self/collaborator/grader) quickly orient to a piece of code. They should describe_ __what__  (_not_ how) _is being done. For more information refer to the [PEP 8 Style Guide for Python](https://www.python.org/dev/peps/pep-0008/)_] The use of docstrings is recommended in all code that is written for this class.\n",
    "\n",
    "* __b) discuss:__ What are the 'keys' and what are the 'values' in this MapReduce job? What delimiter separates them when we write to standard output? How will you expect Hadoop to sort the records emitted by the mapper script? Why is this order important given how the reducer script is written?\n",
    "\n",
    "\n",
    "* __c) run provided code:__ Run the cells provided to make sure that your mapper and reducer scripts are executable, load the input files into HDFS, and clear the HDFS directory where the job will write its output. You will need to do these preparation steps for all future Hadoop MapReduce jobs.\n",
    "\n",
    "\n",
    "* __d) unit test:__ A good habit when writing Hadoop streaming jobs is to test your mappers and reducers locally before passing them to a Hadoop streaming command. An easy way to do this is to pipe in a small line of text. We've provided the unix code to do so and added a unix sort to mimic Hadoop's default sorting. Run these cells to confirm that our mapper and reducer work properly. (Observe how the reducer doesn't work without the sort).\n",
    "\n",
    "\n",
    "* __e) code:__ We've provided the code to run your Hadoop streaming command on the test file. Read through this command and be sure you understand each parameter that we're passing in, then run it and confirm that the output performs word counting correctly. Finally, modify the code provided to run the Hadoop MapReduce job on the _Alice and Wonderland_ text instead of the test file. Remember that the input path you pass to the Hadoop streaming command should be a location in HDFS not a local path. Take a look at the output and confirm you get the same count for 'alice' as in HW1. Food for thought: _does the sorting match what you expected?_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`part c`** Prep for Hadoop Streaming Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - make sure the mapper and reducer are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x WordCount/mapper.py\n",
    "!chmod a+x WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part c - load the input files into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -put {TEST_TXT} {ALICE_TXT} {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part c - clear the output directory (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/wordcount-output\n",
    "# NOTE: this directory won't exist unless you are re-running a job, that's fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`part d`** Unit test your scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part d - unit test mapper script\n",
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part d - unit test reducer script\n",
    "!echo -e \"foo\t1\\nfoo\t1\\nquux\t1\\nlabs\t1\\nfoo\t1\\nbar\t1\\nquux\t1\" | WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part d - systems text mapper and reducer together\n",
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py | WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part d - systems text mapper and reducer together with sort (RUN THIS CELL AS IS)\n",
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py | sort -k1,1 | WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`part e`** Hadoop streaming command. __`NOTE:`__ _don't forget to clear the output directory before re-running this cell (see part b above)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - Hadoop streaming job (RUN THIS CELL AS IS FIRST, then make your modification)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files WordCount/reducer.py,WordCount/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/alice_test.txt \\\n",
    "  -output {HDFS_DIR}/wordcount-output \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - retrieve results from HDFS & copy them into a local file (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/wordcount-output/part-0000* > WordCount/results.txt\n",
    "# NOTE: we would never do this for a really large output file! \n",
    "# (but it's convenient for illustration in this assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part e - view results (RUN THIS CELL AS IS)\n",
    "!head WordCount/results.txt\n",
    "# NOTE: these words and counts should match your results in HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part e - check 'alice' count (RUN THIS CELL AS IS after running the job on the full file)\n",
    "!grep 'alice' WordCount/results.txt\n",
    "# EXPECTED OUTPUT: 404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__DISCUSSION QUESTIONS (after breakout 1)__\n",
    "* What are the 'keys' and what are the 'values' in this MapReduce job? What delimiter separates them when we write to standard output? How will you expect Hadoop to sort the records emitted by the mapper script? Why is this order important given how the reducer script is written?\n",
    "* What was the default sorting you saw? When do you think this sorting happens?\n",
    "* Why go to the trouble of writing the reducer this way? why not just use a dictionary to count the words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout 2: Uppercase and Lowercase Counts\n",
    "\n",
    "What if we didn't care about individual word counts but rather wanted to know how many of the words in the _Alice_ file are upper and lower case? We could retrieve this information easily using a Hadoop streaming job with the same reducer script as in Section 2 (i.e. __`WordCount/reducer.py`__) but a slightly different mapper. In this question you'll design and write your own Hadoop streaming job to do just this.\n",
    "\n",
    "> __DISCUSSION QUESTION:__  \n",
    "> * What should the keys be for this task? [`HINT:` _we'll need a key for each thing we want to count._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 2 Tasks:\n",
    "* __a) code:__ Complete the docstring and code in the __`UpperLower/mapper.py`__ to create a mapper that reads each input line, splits it into words and emits an appropriate key-value pair for each one. [`HINT:` _we're going to use this mapper in conjunction with the reducer from Breakout 1 so your key-value format should look very similar to the one in Breakout 1's mapper._]\n",
    "\n",
    "\n",
    "* __b) unit test:__ Run the provided cells to make your new mapper executable and test that it works as you expect.\n",
    "\n",
    "\n",
    "* __c) code:__ We've provided the start of a Hadoop streaming command. Fill in the missing parameters following the example provided in Section 2. Run your Hadoop job on the test file to confirm that it works. When you are happy with the results replace the test filepath with the real _Alice in Wonderland_ filepath and rerun the job. We'll compare results when we come back from breakouts.\n",
    "\n",
    "* __d) discuss:__ Like our bash script HW1, Hadoop automatically splits up your records to be processed in parallel on separate mapper and reducer \"nodes\" (called \"tasks\" by Hadoop). Judging from the jobs you've run so far, what are the default number of 'map tasks' and 'reduce tasks' that Hadoop uses? Does this framework allow us to directly control the number of mappers and reducers? [__`HINTS`__: _to answer the first part of this question, look at the \"Job Counters\" section in the logging from your Hadoop job; for the second part of this question refer back to Lin & Dyer p24 at the very bottom_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part a - run this cell after completing your portion of the code\n",
    "!chmod a+x UpperLower/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part b - unit test your new mapper (RUN THIS CELL AS IS)\n",
    "!echo \"Foo foo Quux Labs foo bar quux\" | UpperLower/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part b - systems test your new mapper with the reducer from question 2 (RUN THIS CELL AS IS)\n",
    "!echo \"Foo foo Quux Labs foo bar quux\" | UpperLower/mapper.py | sort -k1,1 | WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part c - clear output directory before (re)running your Hadoop Job (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/upperlower-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - Hadoop streaming command (FILL IN MISSING ARGUMENTS)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files UpperLower/mapper.py,WordCount/reducer.py \\\n",
    "\n",
    "    \n",
    "    \n",
    "  -output {HDFS_DIR}/upperlower-output \\ \n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part c - results (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/upperlower-output/part-000* > UpperLower/results.txt\n",
    "!cat UpperLower/results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __DISCUSSION QUESTIONS (after breakout 2)__\n",
    "* How many uppercase/lowercase words did you find?\n",
    "* How many map tasks? how many reduce tasks? Where did you find this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout 3: Number of Unique Words\n",
    "\n",
    "Another variation on the simple word counting job would be to count the number of unique words in the book (instead of counting the unique occurrences of each word). Of course in reality the easiest way to get this information would be to count the number of lines in our word count output file... but since our goal here is to practice designing and writing Hadoop jobs, let's pretend for a moment that we don't have access to that file and instead think about how we'd do this from scratch. In this question we'll also introduce an important flag you can add to your Hadoop streaming jobs to control the degree of parallelization. \n",
    "\n",
    "> __DISCUSSION QUESTIONS:__  \n",
    "* What should our keys be for this task? \n",
    "* Does it make most sense to check for 'uniqueness' inside the mapper or inside the reducer? why? [`HINT:` _think about our discussion of memory constraints in HW1 and about the synchronization that Hadoop performs for us automatically between the map and reduce phases._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 3 Tasks:\n",
    "\n",
    "* __a) code + unit test:__ Since the mapper we wrote for `WordCount` already emits the right keys, let's simply reuse that mapper. Fill in the missing code in __`VocabSize/reducer.py`__so that this new reducer processes the records emitted by __`WordCount/mapper.py`__ and outputs the count of the number of unique words that appear in the input file. Run the provided unit test to confirm that your reducer works as you want it to.\n",
    "\n",
    "\n",
    "* __b) code:__ Write and run a Hadoop streaming job to calculate the number of unique words in _Alice and Wonderland_ and record it in the space provided (`NOTE:` _for 'c' you'll modify this job and overwrite the original result which is why we'll ask you to record it in markdown._)\n",
    "\n",
    "\n",
    "* __c) code + discussion:__ Add the flag `-numReduceTasks 3` to the very end of the Hadoop streaming command you wrote for `part c`. This flag tells Hadoop to use 3 separate reduce tasks, in other words, we'll make 3 'partitions' from the records emitted by your map phase and perform the reducing on each part. Rerun the job with this added flag and observe the result.  \n",
    "    * What do you notice about the contents of the HDFS output directory and the final output itself? How would we have to post process our results to get the answer we're looking for?\n",
    "\n",
    "\n",
    "* __d) Hadoop UI:__ In addition to the logging that Hadoop prints to your notebook you can also access two UIs with more detailed information about your Hadoop streaming jobs. While your job is currently running you can track its progress on port `8088` (this will be especially helpful in latter assignments when we run jobs that may take a long time).The link to this 'Running Job Tracker' UI can be found near the top of the logging from your job. Look for a line that reads something like:\n",
    ">`The url to track the job: http://quickstart.cloudera:8088/proxy/application_########_#####/`\n",
    "\n",
    " Once the job has completed, this link will redirect to the 'MapReduce Job History UI' on port `19888`. This is where you can access information about completed jobs (note the Job ID number will match the one printed in the URL above). \n",
    "  > `localhost:19888/jobhistory/job/job_########_#####`\n",
    "\n",
    " For `part d` Navigate to the MapReduce Job History UI (the one on port `19888`) and confirm that your job used 2 map tasks and 3 reduce tasks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part b - write your code in the provided script first, then RUN THIS CELL AS IS\n",
    "!chmod a+x VocabSize/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part b - unit test your new reducer (RUN THIS CELL AS IS)\n",
    "!echo -e \"foo\t1\\nfoo\t1\\nquux\t1\\nlabs\t1\\nfoo\t1\\nbar\t1\\nquux\t1\" | sort -k1,1 | VocabSize/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part c - clear output directory before (re)running your Hadoop Job (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/vocabsize-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __`TIPS:`__ _When writing your job below make sure that you have the correct paths to your input file, output directory and mapper/reducer script. Don't forget the `-files` option, and_ DO NOT _put spaces between the paths that you pass to this option_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# parts b/c - write/modify your Hadoop streaming command here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# parts b/c - take a look at the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -ls {HDFS_DIR}/vocabsize-output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# parts b/c - view results (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/vocabsize-output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __DISCUSSION QUESTIONS (after breakout 3)__\n",
    "* How many unique words were there?\n",
    "* What happened when 3 reduce tasks were specified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout 4: Secondary Sort\n",
    "In breakout 1 we talked a little bit about the default sorting that Hadoop observes. However we'll often want to sort not just by the key but also by value. For example, we might want to sort the words by their count to find the most frequent words but then break ties by the word in alphabetical order. This is called a 'secondary sort'. In this question we'll learn about specifying parameters for sorting in Hadoop jobs. In particular you'll add three new parameters to your Hadoop Streaming command:  \n",
    "\n",
    "__`-D stream.num.map.output.key.fields=2`__ : tells Hadoop to treat both the first and the second (tab separated) fields as a composite key.  \n",
    "\n",
    "__`-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator`__ : tells Hadoop that we want to make comparisons (for sorting) based on the fields in this composite key\n",
    "\n",
    "__`-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\"`__: Tells Hadoop to perform a reverse numerical sort on the second field in the composite key and then break ties by sorting (alphabetically) on the first field in the composite key.\n",
    "\n",
    "To find the top words in the _Alice in Wonderland_ text we'll use the output of our word counting job as the input for this new sorting task. Recall that this output is a file in alphabetical order whose lines are of the format `word \\t count`. Also recall that this file is already available in HDFS at the path `{HDFS_DIR}/wordcount-output`. You can simply pass this directory path in to the Hadoop streaming input parameter and it will understand that it should read in the directory's contents. __`IMPORTANT`:__ _please use a single reduce task for parts a and b._\n",
    "\n",
    "> __DISCUSSION QUESTIONS (before breakout 4):__   \n",
    "* Before we get to the full secondary sort it's worth noting that there is a really easy way to get Hadoop to sort our file by count: we could just switch the order of the count and the word when we print to standard output in our mapper. Why does this work?\n",
    "* In the Hadoop job below we're using `/bin/cat` as our reducer... what is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 4 Tasks:\n",
    "\n",
    "* __a) code + discussion:__ Complete the code in __`TopWords/mapper.py`__ so that it performs the switch described above. For debugging purposes we'll first run this job using a test file of word counts instead of the full _Alice_ file. Run the provided Hadoop streaming command to confirm that our sneaky solution works. \n",
    "    * Notice that there is a problem with this result. Briefly discuss in groups what the problem is and why it happens.\n",
    "\n",
    "\n",
    "* __b) code:__ Ok, for obvious reasons our 'sneaky' solution didn't quite give us the output we wanted. So let's do a secondary sort properly this time. To do this, add the three new Hadoop options described in the intro to this question. Run your job with these new specifications on the dummy count file. When you are satisfied that your job works, change the input path to specify the alice count output that is already in HDFS. Your list of top words should match the result you got in HW1.\n",
    "    * __`Two important warnings:`__  1) Parameters starting with the `-D` flag must come immediately after the line where you specify the jar file and before the parameters `-files`, `-mapper`, etc; 2) The options we provided you above specify a reverse numeric sort on the second field and tie breaking using the first field... but the mapper you wrote in part a switched the order of the words and the counts. You will need to make a small adjustment to the options we provided so that it instead reverse numerically sorts by the first field and breaks ties on the second. \n",
    "\n",
    "\n",
    "* __c) code + discussion:__ Run your Hadoop job one more time but this time add the parameter to specify that the job should use 2 reduce tasks instead of 1. For convenience of illustration, you should do this using the sample counts file instead of the full _Alice_ text. \n",
    "    * Something is wrong with the results. Use the provided code to look at the output of each partition independently. Discuss why our results are off.  \n",
    "    * Imagine we had a really large file and performed a sort using 100 partitions, what post processing would we have to do to get a fully ordered list (Total Order Sort)? Compare the computational cost of this postprocessing to the postprocessing we discussed in the VocabSize job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ The cell below will create a short file of word counts that we can load into HDFS and use to test our Hadoop MapReduce job. Take a moment to read this sample file and figure out what a reverse numerical sort (with alphabetical tie breaking) should yield. Then go on to complete your tasks as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%writefile TopWords/sample.txt\n",
    "foo\t5\n",
    "quux\t9\n",
    "labs\t100\n",
    "bar\t5\n",
    "qi\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load sample file into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal TopWords/sample.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part a - complete your work above then RUN THIS CELL AS IS\n",
    "!chmod a+x TopWords/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# parts a/b/c - clear output directory (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/topwords-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a/b/c - Hadoop streaming command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files TopWords/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/sample.txt \\\n",
    "  -output {HDFS_DIR}/topwords-output \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part a/b/c - Save results locally (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/topwords-output/part-0000* > TopWords/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part a/b/c - view results (RUN THIS CELL AS IS)\n",
    "!head TopWords/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part c - look at first partition (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/topwords-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part c - look at second partition (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/topwords-output/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected Results:__\n",
    "\n",
    "<table>\n",
    "<th>part a</th>\n",
    "<th>part b</th>\n",
    "<th>part c</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "1\tqi\n",
    "100\tlabs\n",
    "5\tfoo\n",
    "5\tbar\n",
    "9\tquux\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "100\tlabs\n",
    "9\tquux\n",
    "5\tbar\n",
    "5\tfoo\n",
    "1\tqi\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "9\tquux\n",
    "5\tbar\n",
    "100\tlabs\n",
    "5\tfoo\n",
    "1\tqi\n",
    "</pre></td>\n",
    "</tr></table>\n",
    "\n",
    ">__DISCUSSION QUESTIONS__:\n",
    "* What was the problem with the results in part a?\n",
    "* How do you know the secondary sort worked in part b?\n",
    "* What was the problem with the results in part c?\n",
    "* How could we post process these files (part c partitions) to produce a total order sort of them? Why would this be computationally expensive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout 5: Tracking Down Errors in Python Code\n",
    "You've now seen most of the basic functionality of writing and running Hadoop streaming jobs. In this week's homework and over the course of the next few weeks we'll explore additional options and tricks to add to our jobs. As we do this you will want to be able to quickly distinguish between errors that occur due to a mistake in your algorithm design or Hadoop streaming command and errors that are rooted in your Python code. Unfortunately the error logs printed to console do not always make this distinction obvious. Luckily, the Hadoop UI logs do make it very easy to identify Python coding errors. Before you move on to the homework (even if there isn't time in class) we'd like to make sure you know where to find these logs and how to fix two common mistakes. __Below, we provided code that contains two common errors for you to debug. Your job is to:__\n",
    "1. __Run the provided code as is, it will throw an error.__\n",
    "\n",
    "2. __Navigate to the Hadoop UI and find the relevant logs explaining your error.__ \n",
    " * Under `Task Type`, click `Map` > `task_XXXXXX` >`logs`\n",
    "\n",
    "3. __Fix the error(s) and re-run the job__. \n",
    "\n",
    "__`NOTE 1:`__ There are two different kinds of errors in the mapper code. See the inline comments for specific fixes: one involves adding a parameter to your Hadoop job the other two you must fix in the mapper code (re-run that cell to overwrite the old mapper). I'd recommend fixing them one at a time so that you can see how the logs and error messages change depending on the type of error.\n",
    "\n",
    "__`NOTE 2:` If you do not get to this section in class, you still must complete it before beginning the homework!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the following cells to create the demo mapper\n",
    "!mkdir demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell uses a little Jupyter Magic to create a python script on the fly, this is a useful technique that you may want to use in the future to create files for unit testing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting TopWords/sample.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile TopWords/sample.txt\n",
    "foo\t5\n",
    "quux\t9\n",
    "labs\t100\n",
    "bar\t5\n",
    "qi\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put TopWords/sample.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "This is a silly mapper to demonstrate some errors.\n",
    "\"\"\"\n",
    "import sys\n",
    "import numpy as np  # To use numpy add -cmdenv PATH={PATH} to your Hadoop Job\n",
    "\n",
    "for line in sys.stdin:\n",
    "    msg = (\"a message\"    # missing a parenthesis here\n",
    "    print(1/0)            # dividing by zero is a no-go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo2/demo-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# clear HDFS output directory when you re-run the job\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/demo-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob1961176129890797640.jar tmpDir=null\n",
      "20/09/13 14:20:36 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/09/13 14:20:36 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/09/13 14:20:38 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "20/09/13 14:20:38 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/09/13 14:20:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1600006530959_0002\n",
      "20/09/13 14:20:39 INFO impl.YarnClientImpl: Submitted application application_1600006530959_0002\n",
      "20/09/13 14:20:39 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1600006530959_0002/\n",
      "20/09/13 14:20:39 INFO mapreduce.Job: Running job: job_1600006530959_0002\n",
      "20/09/13 14:20:50 INFO mapreduce.Job: Job job_1600006530959_0002 running in uber mode : false\n",
      "20/09/13 14:20:50 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/09/13 14:20:56 INFO mapreduce.Job: Task Id : attempt_1600006530959_0002_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "20/09/13 14:20:57 INFO mapreduce.Job: Task Id : attempt_1600006530959_0002_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "20/09/13 14:21:03 INFO mapreduce.Job: Task Id : attempt_1600006530959_0002_m_000000_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "20/09/13 14:21:04 INFO mapreduce.Job: Task Id : attempt_1600006530959_0002_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "20/09/13 14:21:11 INFO mapreduce.Job: Task Id : attempt_1600006530959_0002_m_000000_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "20/09/13 14:21:12 INFO mapreduce.Job: Task Id : attempt_1600006530959_0002_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "20/09/13 14:21:18 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/09/13 14:21:19 INFO mapreduce.Job: Job job_1600006530959_0002 failed with state FAILED due to: Task failed task_1600006530959_0002_m_000000\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "20/09/13 14:21:19 INFO mapreduce.Job: Counters: 14\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=7\n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tOther local map tasks=6\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39798\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=39798\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=39798\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=40753152\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "20/09/13 14:21:19 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files demo/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/sample.txt \\\n",
    "  -output {HDFS_DIR}/demo-output \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow-Up:  \n",
    "* The below images should be similar to what was found during the above exercise. Before beginning homework 2, make sure you can find these errors through the Hadoop UI. \n",
    "    * If you are unable to find these errors in the Hadoop UI logs, **seek the help of an instructor or TA immediately**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hadoop-module-error](failed-job-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hadoop-module-error](failed-job-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hadoop-module-error](failed-job-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hadoop-module-error](failed-job-04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
