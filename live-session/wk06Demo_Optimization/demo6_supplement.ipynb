{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# wk6 Demo - Supplement\n", "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2019`__\n", "\n", "This week's lesson focused on understanding Gradient Descent as the seminal application of optimization theory. The main demo notebook walked you through a few exercizes to illustrate core concepts. This notebook includes supplemental code related to gradient descent and linear regression.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Notebook Set Up"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# general imports\n", "import sys\n", "import csv\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "# magic commands\n", "%matplotlib inline\n", "%reload_ext autoreload\n", "%autoreload 2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# A Bigger Example\n", "\n", "Ok, now that we have a handle on the math and the basic design choices involved in implementing & parallelizing Gradient Descent. Lets look at a slightly bigger example: the boston housing prices dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ML modules\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import KFold\n", "from sklearn.preprocessing import MinMaxScaler\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.metrics import mean_absolute_error, mean_squared_error"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# set style for plotting\n", "sns.set(style=\"whitegrid\", font_scale=1.3)\n", "matplotlib.rcParams[\"legend.framealpha\"] = 1\n", "matplotlib.rcParams[\"legend.frameon\"] = True"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# we'll also fix the random seed for reproducibility\n", "np.random.seed(42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Boston House Prices Dataset"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Boston dataset is extremely common in machine learning experiments thus it is embedded in sklearn. Run the next few cells to load the data and become familiar with it."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Load the data & take a look.\n", "from sklearn.datasets import load_boston\n", "boston = load_boston()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# OPTIONAL - view description\n", "print(boston.DESCR)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Create data frame & test/train split.\n", "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n", "y = boston.target\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Take a look\n", "X.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exploratory Data Analysis"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Before we jump into our linear regression, its good to become familiar with the variables you will be modeling."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Summary statistics\n", "X.describe()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that some of the features are catecorical and some are continious.\n", "\n", "Let's also take a look at a correlation matrix of features."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# compute the correlation matrix\n", "corr = X.corr()\n", "\n", "# generate a mask for the lower triangle\n", "mask = np.zeros_like(corr, dtype=np.bool)\n", "mask[np.triu_indices_from(mask)] = True\n", "\n", "# set up the matplotlib figure\n", "f, ax = plt.subplots(figsize=(18, 18))\n", "\n", "# generate a custom diverging colormap\n", "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n", "\n", "# draw the heatmap with the mask and correct aspect ratio\n", "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3,\n", "            square=True, \n", "            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Scaling\n", "\n", "Once we start performing gradient descent on real world data, a few additional concerns arise. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Lets visualize two of the features from the Boston Dataset\n", "plt.figure(figsize=(18, 8))\n", "plt.subplot(121)\n", "plt.scatter(X_train.RM, y_train, label=\"Train\")\n", "plt.scatter(X_test.RM, y_test, c=\"r\", label=\"Test\")\n", "plt.xlabel(\"Average number of rooms per dwelling\")\n", "plt.ylabel(\"Price, $\")\n", "plt.legend(loc=\"lower right\", frameon=True)\n", "plt.subplot(122)\n", "plt.scatter(X_train.RAD, y_train, label=\"Train\")\n", "plt.scatter(X_test.RAD, y_test, c=\"r\", label=\"Test\")\n", "plt.xlabel(\"Index of accessibility to radial highways\")\n", "plt.ylabel(\"Price, $\")\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> __DISCUSSION QUESTION:__ What will happen in the Gradient Descent update step when you have input variables that are measured on very different scales? What could we do to avoid this problem?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you probably realized, the easiest solution is to perform normalization before we start modeling. Here we'll normalize each feature so that all $x_{ji} \\in (0,1)$."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["scaler = MinMaxScaler()\n", "X_train = scaler.fit_transform(X_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that we're going to learn normalization constants only on training set. That's done because the assumption is that test set is unreachable during training. However once we've got our normalization functions, we'll also aply them to the test set."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["X_test = scaler.transform(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Sklearn Linear Regression"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here we use very simple Linear Regression model. Scikit-learn uses the closed-form solition for Linear Regression problem thus it gives very good results."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Fitting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# initialize the model\n", "model_sk = LinearRegression()\n", "\n", "# fit the data\n", "model_sk.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# run our model to predict the test and train sets for evaluation\n", "preds_test = model_sk.predict(X_test)\n", "preds_train = model_sk.predict(X_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Evaluation\n", "\n", "Let's see what features are significant for the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12, 8))\n", "plt.bar(np.arange(model_sk.coef_.shape[0]), model_sk.coef_)\n", "plt.xticks(np.arange(model_sk.coef_.shape[0]), X.columns, rotation='vertical')\n", "plt.xlim([-1, model_sk.coef_.shape[0]])\n", "plt.title(\"Sklearn model coefficients\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next we'll evaluate our model according to three different metrics: \n", "* MAE (Mean Absolute Error)\n", "* RMSE (Root Mean Squared Error)\n", "* MAPE (Mean Absolute Percentage Error)\n", "\n", "Note that there is no MAPE implementation in sklearn & that this is a problematic metric because it is prone to ZeroDivisionErrors. However for today's illustration we have included a custom implementation in the supplemental file `linRegFunc.py`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# define a function\n", "def evaluate(models, metrics, samples, metrics_names, models_names):\n", "    \"\"\"\n", "    This function runs Linear Regression Evaluation metrics \n", "    by looping over a provided set of models and datasets.\n", "    \"\"\"\n", "    results = np.zeros((len(samples) * len(models), len(metrics)))\n", "    samples_names = []\n", "    for m in models_names:\n", "        samples_names.extend([m + \" Train\", m + \" Test\"])\n", "    for m_num, model in enumerate(models):\n", "        for row, sample in enumerate(samples):\n", "            for col, metric in enumerate(metrics):\n", "                results[row + m_num * 2, col] = metric(sample[1], model.predict(sample[0]))\n", "    results = pd.DataFrame(results, columns=metrics_names, index=samples_names)\n", "    return results"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# define metrics to run\n", "metrics = [mean_absolute_error, \n", "           lambda y_true, y_pred: mean_squared_error(y_true, y_pred) ** 0.5, \n", "           linRegFunc.mean_absolute_percentage_error]\n", "metrics_names = [\"MAE\", \n", "                 \"RMSE\", \n", "                 \"MAPE\"]\n", "\n", "# define data sets & models to run on\n", "samples = [(X_train, y_train), \n", "           (X_test, y_test)]\n", "models_names = [\"Sklearn\"]\n", "models = [model_sk]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# function call\n", "evaluate(models, metrics, samples, metrics_names, models_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It also interesting to take a look how the predicted points relate to real ones. All the points should lie on the black dotted line ($y=x$) assuming that our model is perfect."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))\n", "plt.scatter(y_train, preds_train, label=\"Train\")\n", "plt.scatter(y_test, preds_test, c=\"r\", label=\"Test\")\n", "plt.xlabel(\"Real price, $\")\n", "plt.ylabel(\"Predicted price, $\")\n", "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=3, label=\"Ideal\")\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cross-validation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The common method to evaluate the model is cross-validation. The idea behind it is to divide the whole set of objects into $k$ sections and then use one section as a test set and other $k-1$ as a train (repeat it with all the sections).\n", "\n", "There is a special function for this in sklearn called $\\text{KFold}$. It creates set of indices for cross-validation."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["cv = KFold(n_splits=5, shuffle=True, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next step is to do everything that we've done before in a loop:\n", "* Split\n", "* Scale\n", "* Train\n", "* Evaluate\n", "\n", "And store the average value of the errors ($\\text{res}$ variable)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["cv.split(X, y=None, groups=None)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["res = None\n", "for train_idx, test_idx in cv.split(X, y=None, groups=None):\n", "    # split\n", "    X_train, X_test = X.values[train_idx], X.values[test_idx]\n", "    y_train, y_test = y[train_idx], y[test_idx]\n", "    \n", "    # scale\n", "    scaler = MinMaxScaler()\n", "    X_train = scaler.fit_transform(X_train)\n", "    X_test = scaler.transform(X_test)\n", "    \n", "    samples_cv = [(X_train, y_train), \n", "                  (X_test, y_test)]\n", "    \n", "    # fit\n", "    model_sk_cv = LinearRegression().fit(samples_cv[0][0], samples_cv[0][1])\n", "    \n", "    # evaluate\n", "    if res is None:\n", "        res = evaluate([model_sk_cv], metrics, samples_cv, metrics_names, [\"Sklearn CV\"])\n", "    else:\n", "        res += evaluate([model_sk_cv], metrics, samples_cv, metrics_names, [\"Sklearn CV\"])\n", "# take the average value across all folds\n", "res /= cv.n_splits"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here is the result of CV"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["res"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Homegrown Linear Regresssion\n", "\n", "In this section we'll write our own Linear Regression class that performs gradient descent. This class will use numpy for efficient matrix calculations. Recall that the matrix representation of our loss functions is: \n", "\n", "$$\n", "f(\\boldsymbol{\\theta}) = \\frac{1}{n}\\|\\text{X}'\\cdot \\boldsymbol{\\theta} - \\mathbf{y}\\|_2^2\n", "$$\n", "\n", "Then the gradient can be easily calculated in vectorized form:\n", "\n", "$$\n", "\\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}) = \\frac{2}{n}\\,\\text{X}'^{\\text{T}}\\left(\\text{X}'\\cdot \\boldsymbol{\\theta} - \\mathbf{y}\\right)\n", "$$\n", "\n", "Exactly these computations are implemented down below in **BasicLinearRegressionHomegrown** class"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["class BasicLinearRegressionHomegrown(object):\n", "    \n", "    def __init__(self):\n", "        self.coef_ = None       # weight vector\n", "        self.intercept_ = None  # bias term\n", "        self._theta = None      # augmented weight vector, i.e., bias + weights\n", "                                # this allows to treat all decision variables homogeneously\n", "        self.history = {\"cost\": [], \n", "                        \"coef\": [], \n", "                        \"intercept\": [], \n", "                        \"grad\": []}\n", "        \n", "    def _grad(self, X, y):\n", "        \"\"\"\n", "        Calculate the gradient of the objective function\n", "\n", "        Args:\n", "            X(ndarray):        train objects\n", "            y(ndarray):        answers for train objects\n", "        Return:\n", "            gradient(ndarray): analytical gradient vector\n", "        \"\"\"\n", "        pred = np.dot(X, self._theta)\n", "        error = pred - y\n", "        gradient = 2 * np.dot(error, X) / X.shape[0]\n", "        return gradient\n", "    \n", "    # full gradient descent, i.e., not stochastic gd\n", "    def _gd(self, X, y, max_iter, alpha):\n", "        \"\"\"\n", "        Runs GD and logs error, weigths, gradient at every step\n", "\n", "        Args:\n", "            X(ndarray):      train objects\n", "            y(ndarray):      answers for train objects\n", "            max_iter(int):   number of weight updates\n", "            alpha(floar):    step size in direction of gradient\n", "        Return:\n", "            None\n", "        \"\"\"\n", "        for i in range(max_iter):\n", "            self.history[\"coef\"].append(self._theta[1:].copy())\n", "            self.history[\"intercept\"].append(self._theta[0].copy())\n", "            \n", "            rmse = self.score(X, y)\n", "            self.history[\"cost\"].append(rmse)\n", "\n", "            # calculate gradient\n", "            grad = self._grad(X, y)\n", "            self.history[\"grad\"].append(grad)\n", "            \n", "            # do gradient step\n", "            self._theta -= alpha * grad\n", "    \n", "    def fit(self, X, y, max_iter, alpha):\n", "        \"\"\"\n", "        Public API for fitting a linear regression model\n", "\n", "        Args:\n", "            X(ndarray):      train objects\n", "            y(ndarray):      answers for train objects\n", "            max_iter(int):   number of weight updates\n", "        Return:\n", "            self\n", "        \"\"\"\n", "        # Augment the data with the bias term.\n", "        # So we can treat the the input variables and the bias term homogeneously \n", "        # from a vectorization perspective\n", "        X = np.c_[np.ones(X.shape[0]), X]\n", "        # initialize if the first step\n", "        if self._theta is None:\n", "            self._theta = np.random.rand(X.shape[1])\n", "        \n", "        # do full gradient descent\n", "        self._gd(X, y, max_iter, alpha)\n", "        \n", "        self.intercept_ = self._theta[0]\n", "        self.coef_ = self._theta[1:]\n", "        return self\n", "        \n", "    def score(self, X, y):\n", "        \"\"\"\n", "        Calculate RMSE metric\n", "\n", "        Args:\n", "            X(ndarray):      objects\n", "            y(ndarray):      answers\n", "        Return:\n", "            rmse(float):     RMSE\n", "        \"\"\"\n", "        pred = self.predict(X)\n", "        error = pred - y\n", "        rmse = (np.sum(error ** 2) / X.shape[0]) ** 0.5\n", "        return rmse\n", "        \n", "    def predict(self, X):\n", "        \"\"\"\n", "        Make a prediction\n", "\n", "        Args:\n", "            X(ndarray):      objects\n", "        Return:\n", "            pred(ndarray):   predictions\n", "        \"\"\"\n", "        # check whether X has appended bias feature or not\n", "        if X.shape[1] == len(self._theta):\n", "            pred = np.dot(X, self._theta)\n", "        else:\n", "            pred = np.dot(X, self.coef_) + self.intercept_\n", "        return pred"]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Create model__"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["model_homegrown = BasicLinearRegressionHomegrown()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Fitting__"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_homegrown.fit(X_train, y_train, max_iter=100, alpha=0.001)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))\n", "plt.plot(model_homegrown.history[\"cost\"], label=\"Train\")\n", "plt.xlabel(\"Iteration\")\n", "plt.ylabel(\"RMSE\")\n", "plt.title(\"Linear Regression via Gradient Descent\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Evaluation__"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["models = [model_sk, model_homegrown]\n", "models_names = [\"Sklearn\", \"Homegrown\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["evaluate(models, metrics, samples, metrics_names, models_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Numerical check\n", "In the last section **BasicLinearRegressionHomegrown** class provides you with the method *_grad* that allows to compute analytical gradient. This function is correct. However with more complicated Loss Functions we may run the risk of making a mistake. In this section we'll look at a simple way to check that our gradient function is right."]}, {"cell_type": "markdown", "metadata": {}, "source": ["The formula for analytical gradient (from calculus):\n", "\n", "$$\n", "\\nabla f(\\mathbf{x}) = \n", "\\begin{bmatrix}\n", "\\frac{\\partial f}{\\partial x_1}\\\\\n", "\\vdots\\\\\n", "\\frac{\\partial f}{\\partial x_m}\n", "\\end{bmatrix}, \\text{ where } m \\text{ is the space dimension}\\\\\n", "\\frac{\\partial f}{\\partial x_1} = \\lim_{\\alpha \\rightarrow 0} \\frac{f(x_1 + \\alpha, x_2 \\ldots x_m) - f(x_1, x_2 \\ldots x_m)}{\\alpha}\n", "$$\n", "\n", "For sufficiently small $\\alpha$ one can approximate partial derivative by simple throwing out the limit operator\n", "\n", "$$\n", "\\frac{\\partial f}{\\partial x_1} \\approx \\frac{f(x_1 + \\alpha, x_2 \\ldots x_m) - f(x_1, x_2 \\ldots x_m)}{\\alpha} = \\left( \\frac{\\partial f}{\\partial x_1} \\right)_{\\text{num}}\\\\\n", "$$\n", "\n", "Then the final approximation of the gradient is:\n", "\n", "$$\n", "\\nabla f(\\mathbf{x}) \\approx \\nabla_{\\text{num}\\,\\,} f(\\mathbf{x}) = \\begin{bmatrix}\n", "\\left( \\frac{\\partial f}{\\partial x_1} \\right)_{\\text{num}}\\\\\n", "\\vdots\\\\\n", "\\left( \\frac{\\partial f}{\\partial x_m} \\right)_{\\text{num}}\n", "\\end{bmatrix}\n", "$$\n", "\n", "The common way of measuring the difference between vectors is the following:\n", "$$\n", "\\text{er} = \\frac{\\|\\nabla f(\\mathbf{x}) - \\nabla_{\\text{num}\\,\\,}f(\\mathbf{x})\\|_2^2}{\\|\\nabla f(\\mathbf{x})\\|_2^2} = \\frac{\\sum_{j=1}^{m}\\left(\\nabla^j f(\\mathbf{x}) - \\nabla^j_{\\text{num}\\,\\,}f(\\mathbf{x})\\right)^2}{\\sum_{j=1}^{m}\\left(\\nabla^j f(\\mathbf{x})\\right)^2}\n", "$$\n", "\n", "The next class, **TweakedLinearRegressionHomegrown**, inherits from **BasicLinearRegressionHomegrown** and adds a method for numerical approximation of gradient. Next we'll \n", "* Check our approximation function by comparing with the analytical one. They **should** be similar.\n", "* Plot the difference of analytical and numerical gradients and describe what we observe."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["class TweakedLinearRegressionHomegrown(BasicLinearRegressionHomegrown):\n", "    \n", "    def __init__(self):\n", "        # call the constructor of the parent class\n", "        super(TweakedLinearRegressionHomegrown, self).__init__()\n", "        self.history[\"grad_num\"] = []\n", "        \n", "    @staticmethod\n", "    def _gradient_approximation(f, x):\n", "        \"\"\"\n", "        Returns the numerical gradient of the function f at the point x\n", "\n", "        Args:\n", "            f(callable): function that takes the point x as an input \n", "                         and returns the value of the function\n", "            x(ndarray): numpy array which contains the coordinates \n", "                        of the point to evaluate gradient\n", "        Return:\n", "            grad_num(ndarray): the numerical approximation \n", "                               of the gradient\n", "        \"\"\"\n", "\n", "        grad_num = np.zeros(len(x))\n", "\n", "        alpha = 0.001\n", "        for i in range(len(x)):\n", "            h = np.zeros(len(x))\n", "            h[i] += alpha\n", "            grad_num[i] = (f(x + h) - f(x)) / alpha\n", "\n", "        return grad_num\n", "    \n", "    def _grad_num(self, X, y):\n", "        \"\"\"\n", "        Returns the numerical gradient of the LinearRegression \n", "        objective function\n", "\n", "        Args:\n", "            X(ndarray): train objects\n", "            y(ndarray): answers for train objects\n", "        Return:\n", "            grad_num(ndarray): the numerical approximation \n", "                               of the gradient\n", "        \"\"\"\n", "        \n", "        grad_num = np.zeros(X.shape[1])\n", "        \n", "        def f(a):\n", "            pred = np.dot(X, a)\n", "            error = pred - y\n", "            mse = np.mean(error ** 2)\n", "            return mse\n", "            \n", "        \n", "        grad_num = self._gradient_approximation(f, self._theta)\n", "        \n", "        return grad_num\n", "    \n", "    def _gd(self, X, y, max_iter, alpha):\n", "        \"\"\"\n", "        Runs GD and logs error, weigths, gradient and \n", "        numerical gradient at every step\n", "\n", "        Args:\n", "            X(ndarray):      train objects\n", "            y(ndarray):      answers for train objects\n", "            max_iter(int):   number of EPOCHS, i.e., full passes over data\n", "            batch_size(int): number of samples in one batch\n", "            alpha(floar):    step size in direction of gradient\n", "        Return:\n", "            None\n", "        \"\"\"\n", "        for i in range(max_iter):\n", "            self.history[\"coef\"].append(self._theta[1:].copy())\n", "            self.history[\"intercept\"].append(self._theta[0].copy())\n", "            \n", "            rmse = self.score(X, y)\n", "            self.history[\"cost\"].append(rmse)\n", "\n", "            grad = self._grad(X, y)\n", "            self.history[\"grad\"].append(grad)\n", "            \n", "            grad_num = self._grad_num(X, y)\n", "            self.history[\"grad_num\"].append(grad_num)\n", "            \n", "            self._theta -= alpha * grad"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create model"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["model_homegrown_check_grad = TweakedLinearRegressionHomegrown()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fitting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_homegrown_check_grad.fit(X_train, y_train, max_iter=100, alpha=0.001)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))\n", "plt.plot(model_homegrown_check_grad.history[\"cost\"], label=\"Train\")\n", "plt.xlabel(\"Iteration\")\n", "plt.ylabel(\"RMSE\")\n", "plt.title(\"Linear Regression via Batch Gradient Descent\")\n", "plt.legend(frameon=True)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plotting error curves"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["grad_num = np.array(model_homegrown_check_grad.history[\"grad_num\"])\n", "grad = np.array(model_homegrown_check_grad.history[\"grad\"])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["def relative_error(grad, grad_num):\n", "    return np.sum((grad - grad_num) ** 2, axis=1) * 1. / np.sum(grad ** 2, axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["def absolute_error(grad, grad_num):\n", "    return np.sum((grad - grad_num) ** 2, axis=1) * 1."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pd.DataFrame(absolute_error(grad, grad_num)).describe()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["absolute_error(grad, grad_num)[:2]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20, 8))\n", "plt.suptitle(\"Numerical approximation of gradient quality\")\n", "plt.subplot(121)\n", "plt.plot(relative_error(grad, grad_num))\n", "plt.xlabel(\"Iteration\")\n", "plt.ylabel(\"Relative error\")\n", "plt.subplot(122)\n", "plt.plot(absolute_error(grad, grad_num))\n", "plt.xlabel(\"Iteration\")\n", "plt.ylabel(\"Absolute error\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As one can observe absolute error jitters but remains approximately the same during all the trining process. The big values in the beginning is due to the fact that the values of gradient is also big.\n", "\n", "Relative error grows because the norm of the gradient (which is in the denominator) becomes smaller while the optimization process converges."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Stochastic Gradient Descent\n", "\n", "In section 3 we suggested that we can get faster convergence if we calculate the gradient not over the whole set of data but over the small (size of $B$) **batch** of it. However with our small dataset this didn't seem to be the case. Let's take a second look now that we are working on a more realistic dataset.\n", "\n", "Recall the gradient calculation for Stoichastic Gradient Descent:\n", "\n", "\\begin{equation}\\tag{3.2}\n", "\\nabla f(\\boldsymbol{\\theta}) \\approx \\nabla_{\\text{batch}\\,\\,} f(\\boldsymbol{\\theta}) = \\frac{2}{n}\\sum_{i=1}^{B}\\left(\\mathbf{x}'_{a_i}\\cdot \\boldsymbol{\\theta} - y_{a_i}\\right)\\cdot \\mathbf{x}'_{a_i}\n", "\\end{equation}\n", "\n", "where $a_i$ is an array of indices of objects which are in this batch. \n", "\n", "The next class **StochasticLinearRegressionHomegrown**, inherits from **TweakedLinearRegressionHomegrown** to stochastic gradient descent algorithm as a member of a class. As before, we'll check that analytical gradient is right via numerical gradient function from **TweakedLinearRegressionHomegrown**."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["len(X)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["class StochasticLinearRegressionHomegrown(TweakedLinearRegressionHomegrown):\n", "    \n", "    def __init__(self):\n", "        # call the constructor of the parent class\n", "        super(StochasticLinearRegressionHomegrown, self).__init__()\n", "    \n", "    def _sgd(self, X, y, max_iter, batch_size, alpha):\n", "        \"\"\"\n", "        Runs Stochastic GD and logs error, weigths, gradient and \n", "        numerical gradient at every step\n", "\n", "        Args:\n", "            X(ndarray):      train objects\n", "            y(ndarray):      answers for train objects\n", "            max_iter(int):   number of EPOCHS, i.e., full passes over data\n", "            batch_size(int): number of samples in one batch\n", "            alpha(floar):    step size in direction of gradient\n", "        Return:\n", "            None\n", "        \"\"\"\n", "        \n", "        for epoch in range(max_iter):\n", "            idxs = np.random.permutation(X.shape[0])\n", "            X = X[idxs]\n", "            y = y[idxs]\n", "            for i in range(0, len(X), batch_size):\n", "                \n", "                self.history[\"coef\"].append(self._theta[1:].copy())\n", "                self.history[\"intercept\"].append(self._theta[0].copy())\n", "                \n", "                rmse = self.score(X, y)\n", "                self.history[\"cost\"].append(rmse)\n", "                \n", "                # calculate gradient\n", "                grad = self._grad(X[i:i + batch_size], y[i:i + batch_size])\n", "                self.history[\"grad\"].append(grad)\n", "                \n", "                # numerical gradient\n", "                grad_num = self._grad_num(X[i:i + batch_size], y[i:i + batch_size])\n", "                self.history[\"grad_num\"].append(grad_num)\n", "            \n", "                # do gradient step\n", "                self._theta -= alpha * grad\n", "        \n", "    def fit(self, X, y, max_iter, batch_size, alpha):\n", "        \"\"\"\n", "        Public API for fitting a linear regression model\n", "\n", "        Args:\n", "            X(ndarray):      train objects\n", "            y(ndarray):      answers for train objects\n", "            max_iter(int):   number of EPOCHS, i.e., full passes over data\n", "            batch_size(int): number of samples in one batch\n", "        Return:\n", "            self\n", "        \"\"\"\n", "        X = np.c_[np.ones(X.shape[0]), X]\n", "        if self._theta is None:\n", "            self._theta = np.random.rand(X.shape[1])\n", "\n", "        self._sgd(X, y, max_iter, batch_size, alpha)\n", "        \n", "        self.intercept_ = self._theta[0]\n", "        self.coef_ = self._theta[1:]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create model"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["model_homegrown_sgd = StochasticLinearRegressionHomegrown()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fitting"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["model_homegrown_sgd.fit(X_train, y_train, batch_size=1, max_iter=100, alpha=0.001)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))\n", "plt.plot(model_homegrown_sgd.history[\"cost\"], label=\"Train\")\n", "plt.xlabel(\"Number of times gradient updated: N/batch_size * max_iter\")\n", "plt.ylabel(\"RMSE\")\n", "plt.title(\"Linear Regression via Stochastic Gradient Descent\")\n", "plt.legend(frameon=True)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plotting error curves"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["grad_num = np.array(model_homegrown_sgd.history[\"grad_num\"])\n", "grad = np.array(model_homegrown_sgd.history[\"grad\"])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20, 8))\n", "plt.suptitle(\"Numerical approximation of gradient quality\")\n", "plt.subplot(121)\n", "plt.plot(relative_error(grad, grad_num))\n", "plt.xlabel(\"Iteration\")\n", "plt.ylabel(\"Relative error\")\n", "plt.subplot(122)\n", "plt.plot(absolute_error(grad, grad_num))\n", "plt.xlabel(\"Iteration\")\n", "plt.ylabel(\"Absolute error\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluation"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["models = [model_sk, model_homegrown, model_homegrown_sgd]\n", "models_names = [\"Sklearn\", \"Homegrown Full GD\", \"Homegrown SGD\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["evaluate(models, metrics, samples, metrics_names, models_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## L1 and L2 regularization for Linear Regression\n", "Regularization is a way of penalizing the model for excessive complexity. It allows us to avoid overfitting.\n", "\n", "There are many ways of doing regularization but these two are the major ones:\n", "* **L2-regularization:**\n", "$$\n", "f(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n}\\left[ (\\mathbf{w}\\cdot\\mathbf{x}_i + b) - y_i\\right]^2 + \\lambda \\sum_{j=1}^{m}w_j^2\n", "$$\n", "or in matrix way:\n", "$$\n", "f(\\mathbf{w}, b) = \\|\\text{X}\\cdot\\mathbf{w} + b\\cdot\\mathbf{1}_n - \\mathbf{y}\\|_2^2 + \\lambda \\|\\mathbf{w}\\|_2^2\n", "$$\n", "* **L1-regularization:**\n", "$$\n", "f(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^{n}\\left[ (\\mathbf{w}\\cdot\\mathbf{x}_i + b) - y_i\\right]^2 + \\lambda \\sum_{j=1}^{m}|w_j|\n", "$$\n", "or in matrix way:\n", "$$\n", "f(\\mathbf{w}, b) = \\|\\text{X}\\cdot\\mathbf{w} + b\\cdot\\mathbf{1}_n - \\mathbf{y}\\|_2^2 + \\lambda \\|\\mathbf{w}\\|_1\n", "$$\n", "\n", "The class below, **RegularizedLinearRegressionHomegrown**, inherits from \n", "**BasicLinearRegressionHomegrown** and encorporates both regularization strategies into the GD solution of Linear Regression. Below we'll examine the problem that arises with L1-regularization as well as its benefits."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["class RegularizedLinearRegressionHomegrown(BasicLinearRegressionHomegrown):\n", "    \n", "    def __init__(self, l1_reg=0.0, l2_reg=0.0):\n", "        # call the constructor of the parent class\n", "        super(RegularizedLinearRegressionHomegrown, self).__init__()\n", "        self.l1_reg = l1_reg\n", "        self.l2_reg = l2_reg\n", "        \n", "    def _grad(self, X, y):\n", "        \"\"\"\n", "        Calculate the gradient of the objective function \n", "        with L1 and L2 regularizations\n", "\n", "        Args:\n", "            X(ndarray):        train objects\n", "            y(ndarray):        answers for train objects\n", "        Return:\n", "            gradient(ndarray): analytical gradient vector\n", "        \"\"\"\n", "        pred = np.dot(X, self._theta)\n", "        error = pred - y\n", "        gradient = 2 * np.dot(error, X) / X.shape[0]\n", "        # penalties only for weights\n", "        gradient[1:] += 2 * self.l2_reg * self._theta[1:] + self.l1_reg * np.sign(self._theta[1:])\n", "        return gradient"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create model"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["model_homegrown_regularized_l2 = RegularizedLinearRegressionHomegrown(l1_reg=0.0, l2_reg=0.1)\n", "model_homegrown_regularized_l1 = RegularizedLinearRegressionHomegrown(l1_reg=1.0, l2_reg=0.0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fitting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_homegrown_regularized_l2.fit(X_train, y_train, max_iter=40000)\n", "model_homegrown_regularized_l1.fit(X_train, y_train, max_iter=40000)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluation"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["models = [model_sk, model_homegrown, model_homegrown_regularized_l2, model_homegrown_regularized_l1]\n", "models_names = [\"Sklearn\", \"Homegrown\", \"Homegrown Regularized L2\", \"Homegrown Regularized L1\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["evaluate(models, metrics, samples, metrics_names, models_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Comparison of regularized models"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12, 8))\n", "plt.bar(np.arange(model_homegrown.coef_.shape[0]) - 0.2, model_homegrown.coef_, width=0.2, label=\"No reg\")\n", "plt.bar(np.arange(model_homegrown_regularized_l2.coef_.shape[0]), model_homegrown_regularized_l2.coef_, width=0.2, label=\"L2\")\n", "plt.bar(np.arange(model_homegrown_regularized_l1.coef_.shape[0]) + 0.2, model_homegrown_regularized_l1.coef_, width=0.2, label=\"L1\")\n", "plt.xticks(np.arange(model_sk.coef_.shape[0]), X.columns, rotation='vertical')\n", "plt.xlim([-1, model_sk.coef_.shape[0]])\n", "plt.title(\"Model coefficients comparison\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"2-norm of weights:\\n\")\n", "print(\"{:10s}{:.2f}\".format(\"No reg:\", np.linalg.norm(model_homegrown.coef_)))\n", "print(\"{:10s}{:.2f}\".format(\"L2:\", np.linalg.norm(model_homegrown_regularized_l2.coef_)))\n", "print(\"{:10s}{:.2f}\".format(\"L1:\", np.linalg.norm(model_homegrown_regularized_l1.coef_)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Number of non-zero coefficients:\\n\")\n", "print(\"{:10s}{:d}\".format(\"No reg:\", np.sum(np.abs(model_homegrown.coef_) > 1e-2)))\n", "print(\"{:10s}{:d}\".format(\"L2:\", np.sum(np.abs(model_homegrown_regularized_l2.coef_) > 1e-2)))\n", "print(\"{:10s}{:d}\".format(\"L1:\", np.sum(np.abs(model_homegrown_regularized_l1.coef_) > 1e-2)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As one can notice L2-regularization greatly reduces the 2-norm of weight vector thereby it prevents overfitting. Regularization constant can be used to control [bias-variance tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html).\n", "\n", "L1-regularization plays the same role but it also has one very important prorepty: it can work as a feature selector (impose sparsity on the coefficient vector). From mathematical perspective it can be explained as follows: L1-norm is the \"closest\" approximation to the L0-norm which explicitly equals to the number of non-zero elements. For sure, the ideal feature selector is L0-norm but one can not simply minimize it because of its computationally intractability (due to its combinatorial nature). Thus people usually use L1-norm for that purpose. More intuitive geometrical interpretation can be found [here](https://www.quora.com/Why-is-L1-regularization-supposed-to-lead-to-sparsity-than-L2) or in any other source (lots of them). For more detailed and deep mathematical explanation one can take a look into [compressed sensing](https://en.wikipedia.org/wiki/Compressed_sensing) method introduced by Terence Tao and David Donoho in the early 2000's."]}, {"cell_type": "markdown", "metadata": {}, "source": ["__A couple nice videos by Ritvik Kharkar:__\n", "\n", "\n", "Ridge regression:\n", "https://www.youtube.com/watch?v=5asL5Eq2x0A\n", "\n", "Lasso regression:\n", "https://www.youtube.com/watch?v=jbwSCwoT51M"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "462px", "width": "252px"}, "navigate_menu": true, "number_sections": true, "sideBar": true, "threshold": 4, "toc_cell": true, "toc_position": {"height": "567px", "left": "0px", "right": "707.4456787109375px", "top": "105px", "width": "243px"}, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 2}