{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wk11 Demo - Recommender Systems; ALS\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__\n",
    "\n",
    "Last week we did pagerank. This week we're doing Alternating Least Squares (ALS) Regression. \n",
    "\n",
    "In class today we'll start with a general discussion of recommender systems, then we'll look at some basic theory of ALS and how it can be prallelized in a map/reduce framework like Spark. \n",
    "\n",
    "We provide the code for a closed-form ridge regression (l2) starting with a single node implementation, distributed implementation, and a mllib implementation, for your reference.\n",
    "\n",
    "By the end of today's demo you should be able to:  \n",
    "* ... __identify__ pros and cons in various RS approaches\n",
    "* ... __describe__ ALS regression \n",
    "* ... __implement__ ALS regression in a distributed fashion.\n",
    "\n",
    "__`Additional Resources:`__ \n",
    "- The code in this notebook was based on several notebooks by Jimi Shanahan   \n",
    "- Recommendation Systems: Techniques, Challenges, Application, and Evaluation https://www.researchgate.net/publication/328640457_Recommendation_Systems_Techniques_Challenges_Application_and_Evaluation_SocProS_2017_Volume_2\n",
    "- Matrix Completion via Alternating Least Square(ALS) https://web.stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf    \n",
    "- Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares https://arxiv.org/pdf/1410.2596.pdf     \n",
    "- Explicit Matrix Factorization: ALS, SGD, and All That Jazz https://blog.insightdatascience.com/explicit-matrix-factorization-als-sgd-and-all-that-jazz-b00e4d9b21ea    \n",
    "- Collaborative Filtering for Implicit Feedback Datasets http://yifanhu.net/PUB/cf.pdf    \n",
    "- Joeran Beel https://www.tcd.ie/research/researchmatters/joeran-beel.php\n",
    "\n",
    "- Collaborative Filtering - RDD-based API https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background discussion\n",
    "\n",
    ">From a research perspective, recommender-systems are one of the most diverse areas imaginable. The areas of interest range from hard mathematical/algorithmic problems over user-centric problems (user interfaces, evaluations, privacy) to ethical and political questions (bias, information bubbles). Given this broad range, many disciplines contribute to recommender-systems research including computer science (e.g. information retrieval, natural language processing, graphic and user interface design, machine learning, distributed computing, high performance computing) the social sciences, and many more. Recommender-systems research can also be conducted in almost every domain including e-commerce, movies, music, art, health, food, legal, or finance. This opens the door for interdisciplinary cooperation with exciting challenges and high potential for impactful work. ~Joeran Beel    \n",
    "*Dr Joeran Beel is an Ussher Assistant Professor in the Artificial Intelligence Discipline at the School of Computer Science & Statistics at Trinity College Dublin. https://www.tcd.ie/research/researchmatters/joeran-beel.php*\n",
    "\n",
    "Most of us are inundated with examples of recommendation systems, from Facebook, to Amazon, to Netflix. So instead of  starting with ‘what they are’,   maybe it’s good to start with a quick discussion about what unspoken assumptions underlie their ubiquity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion questions:\n",
    "* What are some of the political and ethical questions related to RS? What could go wrong? Examples?\n",
    "* What are some assumptions to keep in mind and/or try to avoid when designing RS?\n",
    "* What is the value proposition? who are the stakeholders?\n",
    "* What are some of the areas of expertise involved in designing RS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"types-diagram.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.researchgate.net/publication/328640457_Recommendation_Systems_Techniques_Challenges_Application_and_Evaluation_SocProS_2017_Volume_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RS-comparison-table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender System (RS) as bipartite graph\n",
    "\n",
    "We can think of the recommender problem as a weighted bipartite graph, where one set of nodes represents users, and the other set represents items. \n",
    "\n",
    "* __NODES__ - Each user can be represented by a vector of features, thinkgs like preferences, demographics, traits, etc.. and likewise, our items can also be represented by feature vectors. For example, if our items are movies, then we might have features like genre, director, lead actor, etc... (We'll talk about how these features are derived later).  \n",
    "\n",
    "* __EDGES__ - The edges in our graph could be ratings, a positive or negative indicator, or another continuous measure of preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bipartite-graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based\n",
    "Content-Based systems focus on properties of items. Similarity of items is determined by measuring the similarity in their properties. If a user bought a particular item, then we can recommend similar items. Early recommender systems employed this approach. \n",
    "\n",
    "This approach depends heavily on the similarity metrics beteen items and feature engineering. For movies that might be genre, lead actors, etc. When comparing news articles we might want to perform some topic modeling, TFIDF, cosine similarities, etc..\n",
    "\n",
    "We represent each item as a vector of features, and each user as a vector of these item features, and we compute the cosine similarity between a user and an item to determine if the user will like this item.\n",
    "\n",
    "While it was intuitive and easily interpretable, more effective methods have been developed since. \n",
    "\n",
    "Pros: interpretable, no cold start problem, makes use of implicit data collection .  \n",
    "Cons: creates filter bubble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhood Based\n",
    "As discussed in DDS, we could now take a Nearest Neighbor approach, which is intuitive and simple to reason about. The intuition being that similar people like similar things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion questions:\n",
    "1. How can we measure similarity between people? (HINT: Think about the person node representation)\n",
    "2. What are the challenges with this approach from a theoretical standpoint, as well as a computational one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.2.2 Model-Based Filtering__\n",
    "Model-based techniques make use of data mining and machine learning approaches\n",
    "to predict the preference of a user to an item. These techniques include \n",
    "* association rule mining, (Apriori)\n",
    "* clustering, \n",
    "* decision tree, \n",
    "* artificial neural network, \n",
    "* Bayesian classifier, \n",
    "* regression, \n",
    "* link analysis, and \n",
    "* __latent factor models.__   \n",
    "\n",
    "Among these, __latent factor models__ are the most studied and used model-based techniques.\n",
    "These techniques perform dimensionality reduction over user–item preference matrix\n",
    "and learn latent variables to predict preference of the user to an item in the recommendation\n",
    "process. These methods include:\n",
    "* __matrix factorization__, \n",
    "* singular value decomposition, \n",
    "* probabilistic matrix factorization, \n",
    "* Bayesian probabilistic matrix factorization, \n",
    "* low-rank factorization, \n",
    "* nonnegative matrix factorization, and \n",
    "* latent Dirichlet allocation.   \n",
    "\n",
    "Source: https://www.researchgate.net/publication/328640457_Recommendation_Systems_Techniques_Challenges_Application_and_Evaluation_SocProS_2017_Volume_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection - implicit vs explicit feedback\n",
    "Before we dive in to the methodology, let's talk about how the system is popuated in the first place?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit feeback - users provide ratings.\n",
    "\n",
    "### Discussion questions: \n",
    "* What are some of the limitations of star ratings?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digression: Yahoo experiment -   \n",
    "<img src=\"yahoo-experiment.png\">   \n",
    "* On the left - When users are asked to rate movies from a random list, there are many very low ratings.\n",
    "* On the right - When the user has the freedom to choose what items to rate, instead of giving a low rating, they don’t give a rating at all.    \n",
    "\n",
    "These two distributions are very different. The challenge is that we have this data for training and testing where the \"true\" distribution is like one on the left, but the model we build (user experience) depends on the distribution on the right.\n",
    "We can reframe this challenge in terms of missing data. And what this means, is that we cannot make the assumption that the data is missing at random. The consequence is that we cannot ignore missing values, instead, we need a mechanism for imputing those values.   \n",
    "For more information and how this issue can be addressed, see https://www.youtube.com/watch?v=aKQfUbxU96c marker 6:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit feedback - We can make inferences from users’ behavior. \n",
    "If a user buys a product at Amazon, watches a movie on YouTube, or reads a news article, then the user can be said to “like” this item.\n",
    "\n",
    "### Discussion questions:\n",
    "* What are some of the challenges of this approach?\n",
    "* Ex: If I click on a movie but don't watch it, is that a positive or negative indicator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll limit our implementation to explicit feedback given by users in the form of ratings. We might want to do some preprocessing, like normalization. For example, we might want to subtract the mean of the ratings to account for user bias - some users tend to rate higher than others, and vs. And we may or may not want to impute missing values, as discussed above.\n",
    "\n",
    "We can represent our bipartite graph by a $n\\times m$ \"utility\" matrix $R$ with entries $r_{u,i}$ representing the $i$th item rating by user $u$ with $n$ users and $m$ items.\n",
    "\n",
    "Our goal is to fill in the missing (or previously imputed) values of our matrix with good estimates of future ratings. \n",
    "\n",
    "A common approach for this problem is matrix factorization where we make estimates for the complete ratings matrix $R$ in terms of two matrix \"factors\" $U$ and $V$ which multiply together to form $R$. Where $U$ is the user matrix and $V$ is the item matrix.\n",
    "\n",
    "$$\n",
    "R \\approx UV\n",
    "$$\n",
    "\n",
    "We can estimate $R$ by creating factor matricies with reduced complexity $U\\in\\mathbb{R}^{k,n}$ and $V\"\\in\\mathbb{R}^{k,m}$ with $n$ users, $m$ items, and $k$ factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction \n",
    "If we multiply each feature of the user by the corresponding feature of the movie and add everything together, this will be a good approximation for the rating the user would give that movie.\n",
    "\n",
    "$$\n",
    "r'_{u,i} = \\boldsymbol{u}^{T}_{u}\\boldsymbol{v}_{i} = \\sum_{k} u_{u,k}v_{k,i}\n",
    "$$\n",
    "\n",
    "#### Assumptions\n",
    "- Each user can be described by $k$ attributes or features. For example, feature 1 might be a number that says how much each user likes sci-fi movies; however, they are ambiguous since the model derives them similar to a neural network. So we do not get the interpretability.\n",
    "- Each item (movie) can be described by an analagous set of $k$ attributes or features. To correspond to the above example, feature 1 for the movie might be a number that says how close the movie is to pure sci-fi.\n",
    "\n",
    "These user and item vectors are often called latent vectors or low-dimensional embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion questions:\n",
    "* What is a latent factor? Intuitively? Mathematically?\n",
    "* How many latent factors should we choose? What would it mean if we had 1 latent factor? What about if we had too many? HINT: how does it relate to underfitting and overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MF-01.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Alternating Least Sqaures (ALS)\n",
    "### How can we find U and V to approximate R?\n",
    "* What are we trying to optimize?       \n",
    "* What do we start with?\n",
    "* Explain the 3 components of this loss function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Vert R- U\\cdot V\\Vert ^2 + \\lambda \\Vert U \\Vert ^2 + \\lambda \\Vert V \\Vert ^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is something left out of the notation shown, what is it? (hint: are we really computing the loss for every cell?)\n",
    "* Why wouldn't we use GD on this loss function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that minimizing the joint optimaztion is hard. For one, this function is not convex, so there are local minima we could \"get stuck in\". \n",
    "\n",
    "This is where ALS comes in. It turns out if we constrain $U$ or $V$ to be constant, that this is a convex problem since the multiplicative factor is a constant and it has the same matrix notation as standard least square regression.\n",
    "\n",
    "__STEPS__\n",
    "\n",
    "- Initialize $U_0$ and $V_0$   \n",
    "- Holding $U$ constant, solve for $V_1$ to minimize:\n",
    "\n",
    "$$\n",
    "L(V) = \\Vert R- U_0\\cdot V\\Vert ^2 + \\lambda \\Vert V \\Vert ^2\n",
    "$$\n",
    "\n",
    "- Holding $V$ constant, solve for $U_1$ to minimize:$   \n",
    "\n",
    "$$\n",
    "L(U) = \\Vert R- V_1\\cdot U\\Vert ^2 + \\lambda \\Vert U \\Vert ^2\n",
    "$$\n",
    "\n",
    "- Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion questions:\n",
    "* When we say “solve” for $U_i$ and $V_i$, how is that actually done?\n",
    "* Where are there opportunities to parallelize this training process?\n",
    "* How will we partition the data to avoid shuffling?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"parallel-ALS.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion questions\n",
    "* What data needs to be cached/ broadcast at each phase?\n",
    "* What happens in ‘mappers’ (narrow transformations) & what will happen in aggregation (wide transformations)?\n",
    "* How many shuffles per iteration?\n",
    "* Are there any limitations to this approach?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender-System Evaluation\n",
    ">‘What constitutes a good recommender system and how to measure it’ might seem like a simple question to answer, but it is actually quite difficult.  For many years, the recommender-systems community focused on accuracy. \n",
    ">\n",
    ">Accuracy, in the broader sense, is easy to quantify: numbers like error rates such as the difference between a user’s actual rating of a movie and the previously predicted rating by the recommender system (the lower the error rate, the better the recommender system); or precision, i.e. the fraction of items in a list of recommendations that was actually bought, viewed, clicked, etc. (the higher the precision, the better the recommender system). \n",
    ">\n",
    ">Recently, the community’s attention has shifted to other measures that are more meaningful but also more difficult to measure including __serendipity__, __novelty__, and __diversity__. I contributed to this development by critically analyzing the state of the art [15] ; comparing evaluation metrics (click-through rate, user ratings, precision, recall, …) and methods (online evaluations, offline evaluations, user studies) [13] as well as introducing novel evaluation methods [3].\n",
    ">\n",
    ">Regardless of the metrics used to measure how “good” a recommender system is (accuracy, precision, user satisfaction…), studies report surprisingly inconsistent results on how effective different recommendation algorithms are. For instance, as shown in Figure 2, one of my experiments shows that five news recommendation-algorithms perform vastly different on six news websites [5]. Almost every algorithm performed best on at least one news website. Consequently, the operator of a new news website would hardly know which of the five algorithms is the best to use, because any one could potentially be it.  ~Joeran Beel   \n",
    "*Dr Joeran Beel is an Ussher Assistant Professor in the Artificial Intelligence Discipline at the School of Computer Science & Statistics at Trinity College Dublin.* https://www.tcd.ie/research/researchmatters/joeran-beel.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "app_name = \"wk11_demo\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data\n",
    "https://grouplens.org/datasets/movielens/\n",
    "\n",
    "MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota.\n",
    "\n",
    "This data set consists of: \n",
    "* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
    "* Each user has rated at least 20 movies. \n",
    "* Simple demographic info for the users (age, gender, occupation, zip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data directory if it doesn't already exist\n",
    "!mkdir -p data\n",
    "!curl https://www.dropbox.com/s/yk72grsouyw018l/test.data.txt?dl=1 -o data/test.data\n",
    "!curl http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -o data/data-ml-latest-small.zip\n",
    "!curl http://files.grouplens.org/datasets/movielens/ml-10m.zip -o data/data-ml-10m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/ml*\n",
    "!unzip data/data-ml-latest-small.zip -d data/\n",
    "!unzip data/data-ml-10m.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python - Single Node Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = f\"{PWD}/data/ml-10M100K\"\n",
    "\n",
    "# The MovieLens dataset contains 10000054 ratings and 95580 tags applied to 10681 movies by 71567 users.\n",
    "\n",
    "rating_headers = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_table(path.join(baseDir,'ratings.dat'), sep='::', header=None, names=rating_headers)\n",
    "\n",
    "movie_headers = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_table(path.join(baseDir,'movies.dat'), sep='::', header=None, names=movie_headers)\n",
    "movie_titles = movies.title.tolist()\n",
    "\n",
    "df = movies.join(ratings, on=['movie_id'], rsuffix='_r')\n",
    "del df['movie_id_r']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Q Matrix\n",
    "rp = df.pivot_table(columns=['movie_id'],index=['user_id'],values='rating').fillna(0)\n",
    "rp.head()\n",
    "Q = rp.values\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a binary weight matrix (so the algo focuses on say the movies a user rated during each subproblem \n",
    "# (each user can be view as an atomic problem to be solved) that is solved)\n",
    "W = Q>0.5\n",
    "W[W == True] = 1\n",
    "W[W == False] = 0\n",
    "# To be consistent with our Q matrix\n",
    "W = W.astype(np.float64, copy=False)\n",
    "lambda_ = 0.1 # learning rate\n",
    "n_factors = 100\n",
    "m, n = Q.shape\n",
    "\n",
    "#setup user and movie factor matrices of order n_factors between [0, 5] stars\n",
    "X = 5 * np.random.rand(m, n_factors) \n",
    "Y = 5 * np.random.rand(n_factors, n)\n",
    "X.shape\n",
    "\n",
    "#compute the error (Frobenus norm) where\n",
    "# Q target ratings matrix\n",
    "# X and Y are the factorized matrices\n",
    "# W weight matrix\n",
    "def get_error(Q, X, Y, W):\n",
    "    return np.sum((W * (Q - np.dot(X, Y)))**2)\n",
    "\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-weighted version of ALS (does not work well!)\n",
    "# uses all user item values (as opposed to the subset of actual ratings)\n",
    "n_iterations = 20\n",
    "\n",
    "errors = []\n",
    "for i in range(n_iterations):\n",
    "    X = np.linalg.solve(np.dot(Y, Y.T) + lambda_ * np.eye(n_factors), \n",
    "                        np.dot(Y, Q.T)).T\n",
    "    Y = np.linalg.solve(np.dot(X.T, X) + lambda_ * np.eye(n_factors),\n",
    "                        np.dot(X.T, Q))\n",
    "    print(f'{i}th iteration is completed')\n",
    "    errors.append(get_error(Q, X, Y, W))\n",
    "Q_hat = np.dot(X, Y)\n",
    "\n",
    "print('')\n",
    "print(f'Error of rated movies: {get_error(Q, X, Y, W)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# display plots inline (otherwise it will fire up a separate window)\n",
    "%matplotlib inline\n",
    "plt.plot(errors);\n",
    "plt.ylim([0, 20000]);\n",
    "plt.ylim([0, 20000]);\n",
    "plt.xlabel(\"ALS Iteration\")\n",
    "plt.ylabel(\"Total Squared Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_errors = []\n",
    "for ii in range(n_iterations):\n",
    "    for u, Wu in enumerate(W):\n",
    "        #AX=B =>  X=A^-1B ; in python use solve(A, B) \n",
    "        X[u] = np.linalg.solve(np.dot(Y, np.dot(np.diag(Wu), Y.T)) + lambda_ * np.eye(n_factors),\n",
    "                               np.dot(Y, np.dot(np.diag(Wu), Q[u].T))).T\n",
    "    for i, Wi in enumerate(W.T):\n",
    "        Y[:,i] = np.linalg.solve(np.dot(X.T, np.dot(np.diag(Wi), X)) + lambda_ * np.eye(n_factors),\n",
    "                                 np.dot(X.T, np.dot(np.diag(Wi), Q[:, i])))\n",
    "    weighted_errors.append(get_error(Q, X, Y, W))\n",
    "    print(f'{ii}th iteration is completed')\n",
    "weighted_Q_hat = np.dot(X,Y)\n",
    "print(f'Error of rated movies: {get_error(Q, X, Y, W)}')\n",
    "plt.plot(weighted_errors);\n",
    "plt.xlabel('Iteration Number');\n",
    "plt.ylabel('Mean Squared Error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark - Distributed Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "from numpy import matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def rmse(R, U, V): # Metric\n",
    "    return np.sqrt(np.sum(np.power(R-U*V, 2))/(U.shape[0]*V.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def solver(mat, R, LAMBDA):  # solver to get R*mat(matT*mat + lambda*I)^-1\n",
    "    d1 = mat.shape[0]\n",
    "    d2 = mat.shape[1]\n",
    "\n",
    "    X2 = mat.T * mat\n",
    "    XY = mat.T * R.T\n",
    "\n",
    "    for j in range(d2):\n",
    "        X2[j, j] += LAMBDA * d1\n",
    "\n",
    "    return np.linalg.solve(X2, XY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Not only caculation is paralleized but also the data is wisely partitioned and shared to improve locality.\n",
    "def closedFormALS(R,InitialU,InitialVt,rank,iterations,numPartitions,LAMBDA=0.01):\n",
    "    R_Userslice = sc.parallelize(R,numPartitions).cache() # R will automaticly be partitioned by row index\n",
    "    R_Itemslice = sc.parallelize(R.T,numPartitions).cache() # R_T will automaticly be partitioned by row index\n",
    "    U = InitialU\n",
    "    Vt = InitialVt\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        print(f\"Iteration: {i}\")\n",
    "        print(f\"RMSE: {rmse(R, U, Vt.T)}\")\n",
    "        \n",
    "        Vtb = sc.broadcast(Vt)\n",
    "        U3d = R_Userslice.map(lambda x:solver(Vtb.value,x,LAMBDA)).collect() # a list of two 2-D matrix\n",
    "        U = matrix(np.array(U3d)[:, :, 0]) # transfered to 2-D matrix\n",
    "        \n",
    "        Ub = sc.broadcast(U)\n",
    "        Vt3d = R_Itemslice.map(lambda x:solver(Ub.value,x,LAMBDA)).collect() # a list of two 2-D matrix\n",
    "        Vt = matrix(np.array(Vt3d)[:, :, 0])  # transfered to 2-D matrix\n",
    "    \n",
    "    return U, Vt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Only parallelize the calculation. It does not consider the data transmission cost\n",
    "def simpleParalleling(R,InitialU,InitialVt,rank,iterations,numPartitions,LAMBDA=0.01):\n",
    "    Rb = sc.broadcast(R)\n",
    "    U = InitialU\n",
    "    Vt = InitialVt\n",
    "    Ub = sc.broadcast(U)\n",
    "    Vtb = sc.broadcast(Vt)\n",
    "    numUsers = InitialU.shape[0]\n",
    "    numItems = InitialVt.shape[0]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        print(f\"Iteration: {i}\")\n",
    "        print(f\"RMSE: {rmse(R, U, Vt.T)}\")\n",
    "        U3d = sc.parallelize(range(numUsers), numPartitions) \\\n",
    "           .map(lambda x: solver( Vtb.value, Rb.value[x, :],LAMBDA)) \\\n",
    "           .collect() # a list of two 2-D matrix\n",
    "        U = matrix(np.array(U3d)[:, :, 0]) # transfered to 2-D matrix\n",
    "        Ub = sc.broadcast(U)\n",
    "\n",
    "        Vt3d = sc.parallelize(range(numItems), numPartitions) \\\n",
    "           .map(lambda x: solver(Ub.value, Rb.value.T[x,:],LAMBDA)) \\\n",
    "           .collect() # a list of two 2-D matrix\n",
    "        Vt = matrix(np.array(Vt3d)[:, :, 0]) # transfered to 2-D matrix\n",
    "        Vtb = sc.broadcast(Vt)\n",
    "    return U, Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    LAMBDA = 0.01   # regularization parameter\n",
    "    np.random.seed(100)\n",
    "    numUsers = 5000\n",
    "    numItems = 100\n",
    "    rank = 10\n",
    "    iterations = 5\n",
    "    numPartitions = 4\n",
    "\n",
    "    trueU = matrix(rand(numUsers, rank)) #True matrix U to generate R\n",
    "    trueV = matrix(rand(rank, numItems)) #True matrix V to generate R\n",
    "    R = matrix(trueU*trueV)   #generate Rating matrix\n",
    "    \n",
    "    InitialU = matrix(rand(numUsers, rank)) #Initialization of U\n",
    "    InitialVt = matrix(rand(numItems,rank)) #Initialization of V\n",
    "    \n",
    "    print(f\"Running ALS with numUser={numUsers}, numItem={numItems}, rank={rank}, iterations={n_iterations}, numPartitions={numPartitions}\")\n",
    "    \n",
    "    print(\"Distributed Version---Two copies of R, one is partitioned by rowIdx, the other is partitioned by colIndx\")\n",
    "    closedFormALS(R,InitialU,InitialVt,rank,n_iterations,numPartitions,LAMBDA)\n",
    "    \n",
    "    print(\"Simple paralleling ---Suppose User Matrix R is small enough to be broadcast\")\n",
    "    simpleParalleling(R,InitialU,InitialVt,rank,n_iterations,numPartitions,LAMBDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML implementation of ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MlLib Tutorial on Personalized Movie Recommendation\n",
    "Link to Docs: https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html\n",
    "\n",
    "* What are the parameters for the MLLib implementation of collaborative filtering?\n",
    "* How do you determine the ‘rank’ of your latent space vectors (i.e. number of latent factors)?\n",
    "* After training your CF model a new user joins your platform. What will you need to do to generate predictions for that user?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some implementation details\n",
    "\n",
    "Solve for U and V for maxIterations\n",
    "https://github.com/apache/spark/blob/e1ea806b3075d279b5f08a29fe4c1ad6d3c4191a/mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala#L1001\n",
    "```\n",
    "for (iter <- 0 until maxIter)\n",
    "    itemFactors = computeFactors(userFactors, userOutBlocks, itemInBlocks, rank, regParam,\n",
    "          userLocalIndexEncoder, solver = solver)\n",
    "    userFactors = computeFactors(itemFactors, itemOutBlocks, userInBlocks, rank, regParam,\n",
    "          itemLocalIndexEncoder, solver = solver)      \n",
    "```\n",
    "\n",
    "where the default non-negative solver is the `ML` NNLSSolver (non-negative least squares solver)     \n",
    "https://github.com/apache/spark/blob/e1ea806b3075d279b5f08a29fe4c1ad6d3c4191a/mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala#L767\n",
    "\n",
    "which calls `MLLIB` NNLS Solver...\n",
    "which implements the [conjugate gradient method](https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n",
    "https://github.com/apache/spark/blob/e1ea806b3075d279b5f08a29fe4c1ad6d3c4191a/mllib/src/main/scala/org/apache/spark/mllib/optimization/NNLS.scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explain why the function is not convex. https://www.quora.com/Why-is-the-matrix-factorization-optimization-function-in-recommender-systems-not-convex\n",
    "\n",
    ">A function f(x) is said to be convex if it satisfies the following property:\n",
    "\n",
    ">𝑓(𝛼𝑥+𝛽𝑦)≤𝛼𝑓(𝑥)+𝛽𝑓(𝑦) where, 𝛼+𝛽=1,𝛼,𝛽≥=0and the domain of 𝑓 is a convex set.\n",
    "\n",
    ">A simple matrix factorization based model would predict, say ratings, using the product of item and user latent factors\n",
    "\n",
    ">𝑅𝑢𝑖=<𝑝𝑢,𝑞𝑖> where 𝑝𝑢 is the user latent factor representation and 𝑞𝑖is the item latent factor representation. The objective function includes this term and therefore is equivalent to minimizing 𝑓(𝑥,𝑦)=𝑥𝑦\n",
    "\n",
    ">𝑓(𝛼𝑥1+𝛽𝑥2,𝛼𝑦1+𝛽𝑦2)=(𝛼𝑥1+𝛽𝑥2)(𝛼𝑦1+𝛽𝑦2)You can easily find a counter example to prove that this function does not satisfy the above property of convexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acronym Disambiguation\n",
    "Factoring matrices comes up a lot in the context of ML.\n",
    "\n",
    "- __SVD__  - “Singular Value Decomposition”\n",
    "- __PCA__ - “Principal Component Analysis” \n",
    "- __FM__ - “Factorization Machine” (one latent vector per user or item)\n",
    "- __FFM__ - “Field Aware Factorization Machine” (multiple latent vectors depending on the latent space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALS is tolerant of missing values.  \n",
    "SVD requires all values to be present.   \n",
    "To calculate PCA, one would perfom SVD. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netflix Recommender system\n",
    "https://www.youtube.com/watch?v=aKQfUbxU96c\n",
    "\n",
    "SVD++ (uses both explicit and implict feedback, takes into account user and item bias)     \n",
    "Restricted Bolzman Machine   \n",
    "Nuclear Norm -> $||A||_{nuclear} = \\sigma_1 + \\sigma_2 + ... + \\sigma_r$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for training explicit matrix factorization model using either ALS or SGD\n",
    "\n",
    "https://gist.github.com/EthanRosenthal/a293bfe8bbe40d5d0995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RS as a bi-partite graph - how do we solve this in graphX?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep-Learning MF\n",
    "In recent years a number of neural and deep-learning techniques have been proposed, some of which generalize traditional Matrix factorization algorithms via a non-linear neural architecture [15]. While deep learning has been applied to many different scenarios: context-aware, sequence-aware, social tagging etc. its real effectiveness when used in a simple Collaborative filtering scenario has been put into question. A systematic analysis of publications applying deep learning or neural methods to the top-k recommendation problem, published in top conferences (SIGIR, KDD, WWW, RecSys), has shown that on average less than 40% of articles are reproducible, with as little as 14% in some conferences. Overall the study identifies 18 articles, only 7 of them could be reproduced and 6 of them could be outperformed by much older and simpler properly tuned baselines. The article also highlights a number of potential problems in today's research scholarship and calls for improved scientific practices in that area.[16] Similar issues have been spotted also in sequence-aware recommender systems.[17] https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
