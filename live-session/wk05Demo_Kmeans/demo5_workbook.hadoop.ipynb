{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Demo 5 - K-Means Clustering in Hadoop and Spark\n", "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2019`__\n", "\n", "Designing MapReduce algorithms involves two kinds of planning. First we have to figure out which parts of a calculation can be performed in parallel and which can't. Then we have to figure out how to put those pieces together so that the right information ends up in the right place at the right time. That's what the Hadoop shuffle is all about. Today we'll talk about a few techniques to optimize your Hadoop jobs as well as some of the limitations of this framework.\n", "By the end of this demo you should be able to:  \n", "* ... __implement__ K-Means using Hadoop and Spark RDDs.\n", "* ... __explain__ how the centroid initialization affects K-Means results & time to convergence.\n", "* ... __list__ a few limitations of the Hadoop Framework.\n", "\n", "__NOTE__: _At the end of this notebook we've provided a Spark implementation that fixes some of our frustrations with Hadoop. This is intended to set the stage for next week when we'll start a more formal introduction to Spark_."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Notebook Set-Up"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# imports\n", "import sys\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from sklearn.decomposition import PCA\n", "%reload_ext autoreload\n", "%autoreload 2"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# globals\n", "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\"\n", "HOME_DIR = \"/media/notebooks\" # this is where docker mounts your repo, ADJUST AS NEEDED\n", "DEMO_DIR = HOME_DIR + \"/LiveSessionMaterials/wk04Demo_Kmeans\"\n", "HDFS_DIR = \"/user/root/demo4\"\n", "!hdfs dfs -mkdir {HDFS_DIR}"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# store notebook environment path\n", "from os import environ\n", "PATH  = environ['PATH']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["__`REMINDER:`__ If you are running this notebook from the course Docker container you can track your Hadoop Jobs using the UI at: http://localhost:19888/jobhistory/"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Content Review: Kmeans\n", "\n", "K-Means clustering is an algorithm designed to group examples into K distinct groups based on their features. K-Means clustering can be used to categorize data when we do not have any concrete information about what those categories may look like. For example we may want to perform a market segmentation analysis on our customers but we don't actually know what the segments are.\n", "\n", "At a high level clustering is an attempt to group objects in such a way as to make those objects similar to other objects within the group and dissimilar to those outside of the group. In order to perform K-Means clustering we want to minimize the distance between all of the examples that are a part of a cluster and the center of that cluster, also called the _cluster centroid_. The clustering algorithm solves the following cost minimization:\n", "\n", "$$\n", "\\arg\\min_{c} \\sum_{i=1}^k\\sum_{{x}\\in c_i} \\left\\Vert {x}-\\mu_i \\right\\Vert^2\n", "$$\n", "\n", "It turns out that the above minimization problem is very difficult to solve (t falls into the class of problems known as [NP-Hard](https://en.wikipedia.org/wiki/NP-hardness)). The K-Means algorithm attempts to solve this clustering problem iteratively by repeating the 2-step process of marking each example as belonging to a single cluster by finding the closest cluster centroid\n", "and then adjusting each of the K cluster centroids so that each centroid is in the middle of all the examples that belong to that centroid.\n", "\n", "[Here](http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html) is a simple visualization of the process. There are a number of up front choices that must be made before the algorithm can be implemented:\n", "\n", "* __K__: the total number of centroids is selected using some apriori information about the desired outcome\n", "* __Distance Function__: typically the euclidian distance is used but this can be any function $d({x}, \\mu)$\n", "* __Convergence Criteria__: a rule used to determine when the iterative process can stop.\n", "\n", "The formal algorithm can then be defined as follows. First, decide the number of clusters $K$. Then:\n", "\n", "\n", "| Step | Description | Pseudocode|        \n", "| :---: |:---------------------------------------------: |:--------------- |\n", "|1 |Initialize the center ('centroid') of the clusters| ${\\mu}_i = $ random value $, i=1,...,k$|\n", "|2|Attribute the closest cluster to each data point|${c}_i = \\{j: d({x}_j, \\mu_i) \\le d({x}_j, \\mu_l),  l \\ne i, j=1,...,n\\}$|\n", "|3| Update centroid to the mean of points in that cluster|$\\mu_i = \\frac{1}{|c_i|}\\sum_{j\\in c_i} {x}_j,\\forall i$|\n", "|4|Repeat steps 2-3 until convergence criteria met||\n", "||Notation:|${|c|} = $ number of elements in  ${c}$|"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Load Demo Data for K-means\n", "\n", "The Iris dataset was used in R.A. Fisher's classic 1936 paper, [The Use of Multiple Measurements in Taxonomic Problems](http://rcs.chemometrics.ru/Tutorials/classification/Fisher.pdf). It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n", "\n", "The columns in this dataset are:\n", "> `Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species`\n", "\n", "If you want to learn more about this dataset visit the [UCI Iris Data Set page](https://archive.ics.uci.edu/ml/datasets/iris). Use the cells below to download, preprocess and visualize this data set."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# make data directory\n", "!mkdir data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# download the iris dataset\n", "!curl -L -O https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# a few data cleaning steps... (remove blank last line, & separate features from labels)\n", "!head -n -1 iris.data > data/iris_full.csv\n", "!cut -d ',' -f 1-4 data/iris_full.csv > data/iris_features.csv\n", "!rm iris.data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# take a look at the first few lines\n", "!head data/iris_full.csv"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# copy the iris data to hdfs for use with hadoop streaming in subsequent exercises\n", "!hdfs dfs -copyFromLocal data/iris_features.csv {HDFS_DIR}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# set up constants for ease of directory use\n", "IRIS_DATA_WITH_LABELS = DEMO_DIR + '/data/iris_full.csv'\n", "IRIS_DATA = DEMO_DIR + '/data/iris_features.csv'\n", "IRIS_DATA_HDFS = HDFS_DIR + '/iris_features.csv'\n", "OUTPUT_HDFS = HDFS_DIR + \"/new_centroids\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Visualize Demo Data for Kmeans\n", "\n", "For purposes of visualization of this dataset we use a PCA decomposition to project the 4 dimensional iris dataset down to 2 dimensions. This will give us the ability to roughly visualize the effectiveness of our K-Means clustering."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# read in labeled data\n", "mapping = {b'Iris-setosa': 0, \n", "           b'Iris-versicolor': 1, \n", "           b'Iris-virginica': 2}\n", "names = mapping.keys()\n", "data = np.loadtxt(IRIS_DATA_WITH_LABELS, delimiter=',', \n", "                  converters = {4: lambda s: mapping[s]})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# custom plotting function\n", "from utils import plot_iris_data\n", "plot_iris_data(data[:,0:-1], data[:,-1], names, 'Iris Dataset (true labels)')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# K-Means in Python"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Implementation\n", "A simple (non-scalable) implementation in Python. For this example we will use the Iris dataset without labels and a K of 3 since we know apriori there are 3 different species labels (setosa, versicolor, and virginica)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "# read in the data from the IRIS_DATA file\n", "samples = np.loadtxt(IRIS_DATA, \n", "                     delimiter=',')\n", "\n", "# define the number of clusters\n", "k = 3\n", "\n", "# define the distance function as the normalized distance\n", "def distance(a, b, ax=1):\n", "    return np.linalg.norm(a - b, axis=ax)\n", "\n", "def closestCluster(sample, centers):\n", "    distances = distance(sample, centers)\n", "    return np.argmin(distances)\n", "\n", "# define the convergence criteria\n", "convergenceCriteria = .001\n", "\n", "# (step1) set the k initial clusters by assigning a random point\n", "centroids = samples[np.random.choice(len(samples), \n", "                                     size=k, \n", "                                     replace=False)]\n", "\n", "while True:\n", "    # (step2) attribute the closest cluster to each point\n", "    clusters = [closestCluster(sample, centroids) \n", "                for sample \n", "                in samples]\n", "    \n", "    # (step3) set the position of each cluster to the mean of all data points belonging to that cluster\n", "    last_centroids = np.copy(centroids)\n", "    for i in range(k):\n", "        points = [samples[j] \n", "                  for j \n", "                  in range(len(samples)) if clusters[j] == i]\n", "        centroids[i] = np.mean(points, axis=0)\n", "    \n", "    # (step4) repeat steps 2-3 until convergence criteria met\n", "    error = distance(centroids, \n", "                     last_centroids, \n", "                     None)\n", "    if error < convergenceCriteria:\n", "        break\n", "\n", "print('-------Final Centroids-------')\n", "print(centroids)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Results\n", "Examine the results by assigning each of the datapoints in the file to the nearest cluster and then plot all of the samples along with their labels."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["labels = list(map(lambda sample: closestCluster(sample, centroids), samples))\n", "plot_iris_data(np.array(samples), np.array(labels), ['cluster1', 'cluster2', 'cluster3'],\n", "                'Iris Dataset (Python K-Means clustering)')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# K-Means in Hadoop MapReduce\n", "\n", "Topics to cover\n", "* Decomposition of algorithm into map and reduce phases\n", "* Sorting and how shuffle works with this"]}, {"cell_type": "markdown", "metadata": {"ExecuteTime": {"end_time": "2018-03-20T15:18:41.309446Z", "start_time": "2018-03-20T15:18:41.306442Z"}}, "source": ["## Mapper\n", "The primary job of the mapper is to classify points based on the given clusters. The clusters are randomly initialized (as in the previous Python example) but due to the distributed nature of Hadoop those centroid locations cannot be stored in memory. Instead we write them to a file and read them at the beginning of the map phase. The mapper then determines the nearest centroid and subsequently emits the sample point along with the centroid that point belongs to. In addition we emit a 'count' value which will serve a purpose when we look at integrating a combiner. The output format is then\n", "\n", "$(index, count, point)$\n", "\n", "Where the index is the key to be aggregated by the reducer"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["%%writefile mapper.py\n", "#!/usr/bin/env python\n", "import sys, math\n", "\n", "CLUSTERS_FILENAME = 'centroids.txt'\n", "\n", "centroid_points = [list(map(float, s.split('\\n')[0].split(','))) \n", "                   for s \n", "                   in open(CLUSTERS_FILENAME, 'r').readlines()]\n", "\n", "for line in sys.stdin:\n", "    data = [float(x) \n", "            for x \n", "            in line.strip().split(',')]\n", "\n", "    minDistance, index = 0, -1\n", "    for i in range(len(centroid_points)):\n", "        centroid = centroid_points[i]\n", "        distance = sum([(centroid[ix]-data[ix])**2 \n", "                    for ix \n", "                    in range(len(data))])**2\n", "        if minDistance:\n", "            if distance < minDistance:                    \n", "                minDistance, index = distance, i\n", "        else:\n", "            minDistance, index = distance, i\n", "    print(f\"{index}\\t{1}\\t{line.strip()}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!chmod a+x mapper.py"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Reducer\n", "Each reducer will collect all the samples belonging to one or more clusters and subsequently calculate and then emit the new cluster centroids."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["%%writefile reducer.py\n", "#!/usr/bin/env python\n", "import sys, re, math\n", "from operator import add\n", "\n", "oldKey = None\n", "sumOfCoords = None\n", "countOfCoords = 0\n", "\n", "emit = lambda i, s, c: '\\t'.join([str(i), ','.join([str(x/c) for x in s])])\n", "\n", "for line in sys.stdin:\n", "    line = line.strip().split('\\t')\n", "    index, count, coords  = int(line[0]), int(line[1]), [float(x) \n", "                                                         for x \n", "                                                         in line[2].split(',')]\n", "    if oldKey is not None and oldKey != index:\n", "        print(emit(oldKey, sumOfCoords, countOfCoords))\n", "        sumOfCoords = None\n", "        countOfCoords = 0\n", "    \n", "    oldKey = index\n", "    sumOfCoords = map(add, sumOfCoords, coords) if sumOfCoords else coords\n", "    countOfCoords += int(count)\n", "\n", "if oldKey != None:\n", "    print(emit(oldKey, sumOfCoords, countOfCoords))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!chmod a+x reducer.py"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Initializer\n", "Each of the $K$ centroids are initialized to a random point from the dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "k = 3\n", "\n", "samples = np.loadtxt(IRIS_DATA, \n", "                     delimiter=',')\n", "centroids = samples[np.random.choice(len(samples), \n", "                                     size=k, \n", "                                     replace=False)]\n", "np.savetxt('centroids.txt', centroids, fmt='%.2f', delimiter=',')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!cat centroids.txt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Unit Test"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!cat iris-cut.data | ./mapper.py | sort -k 1,1 | ./reducer.py"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Run"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Hadoop streaming command\n", "!hdfs dfs -rm -r {OUTPUT_HDFS}\n", "!hadoop jar {JAR_FILE} \\\n", "  -files mapper.py,reducer.py,centroids.txt \\\n", "  -mapper mapper.py \\\n", "  -reducer reducer.py \\\n", "  -input {IRIS_DATA_HDFS} \\\n", "  -output {OUTPUT_HDFS} \\\n", "  -cmdenv PATH={PATH}"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["!hdfs dfs -cat {OUTPUT_HDFS}/part-0000*"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Adding a Combiner\n", "This particular K-Means implementation can benefit greatly from the addition of a combiner. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%writefile combiner.py\n", "#!/usr/bin/env python\n", "import sys, re, math\n", "from operator import add\n", "\n", "oldKey = None\n", "sumOfCoords = None\n", "countOfCoords = 0\n", "\n", "emit = lambda i, s, c: '\\t'.join([str(i), str(c), ','.join([str(x) for x in s])])\n", "\n", "for line in sys.stdin:\n", "    line = line.strip().split('\\t')\n", "    index, count, coords  = int(line[0]), int(line[1]), [float(x) \n", "                                                         for x \n", "                                                         in line[2].split(',')]\n", "    if oldKey is not None and oldKey != index:\n", "        print(emit(oldKey, sumOfCoords, countOfCoords))\n", "        sumOfCoords = None\n", "        countOfCoords = 0\n", "    \n", "    oldKey = index\n", "    sumOfCoords = map(add, sumOfCoords, coords) if sumOfCoords else coords\n", "    countOfCoords += int(count)\n", "\n", "if oldKey != None:\n", "    print(emit(oldKey, sumOfCoords, countOfCoords))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!chmod a+x combiner.py"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Run with Combiner"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Hadoop streaming command\n", "!hdfs dfs -rm -r {OUTPUT_HDFS}\n", "!hadoop jar {JAR_FILE} \\\n", "  -files mapper.py,combiner.py,reducer.py,centroids.txt \\\n", "  -mapper mapper.py \\\n", "  -combiner combiner.py \\\n", "  -reducer reducer.py \\\n", "  -input {IRIS_DATA_HDFS} \\\n", "  -output {OUTPUT_HDFS} \\\n", "  -cmdenv PATH={PATH}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!hdfs dfs -cat {OUTPUT_HDFS}/part-0000*"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Discussion"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is only a single iteration. How do we accomplish multiple iterations with Hadoop streaming?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# K-Means in Spark"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is boilerplate code required for a notebook in this enviroment to create a local spark context."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql import SparkSession\n", "\n", "app_name = \"example_notebook\"\n", "master = \"local[*]\"\n", "spark = SparkSession\\\n", "        .builder\\\n", "        .appName(app_name)\\\n", "        .master(master)\\\n", "        .getOrCreate()\n", "sc = spark.sparkContext"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Implementation\n", "Compare this Spark K-Means implementation to the original, non-scalable implementation in Python. Structurally they are quite similar."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["import numpy as np\n", "\n", "lines = sc.textFile(IRIS_DATA)\n", "samples = lines.map(lambda line: np.array([float(x) for x in line.split(',')])).cache()\n", "\n", "# define the number of clusters\n", "k = 3\n", "\n", "# define the distance function as the normalized distance\n", "def distance(a, b):\n", "    return np.linalg.norm(a - b)\n", "\n", "def closestCluster(p, centers):\n", "    bestIndex = 0\n", "    closest = float(\"+inf\")\n", "    for i in range(len(centers)):\n", "        tempDist = distance(p, centers[i])\n", "        if tempDist < closest:\n", "            closest = tempDist\n", "            bestIndex = i\n", "    return bestIndex\n", "\n", "# define the convergence criteria\n", "convergenceCriteria = .001\n", "\n", "# (step1) set the k initial clusters by assigning a random point\n", "centroids = samples.takeSample(False, k, 1)\n", "\n", "while True:\n", "    # (step2) attribute the closest cluster to each point\n", "    clusters = samples.map(\n", "        lambda p: (closestCluster(p, centroids), (p, 1)))\n", "    \n", "    newpoints = clusters.reduceByKey(\n", "        lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1])).map(\n", "        lambda st: (st[0], st[1][0] / st[1][1])).collect()\n", "    \n", "    error = sum(np.sum(distance(centroids[iK], p)) for (iK, p) in newpoints)\n", "        \n", "    # (step3) set the position of each cluster to the mean of all data points belonging to that cluster\n", "    for (iK, p) in newpoints:\n", "        centroids[iK] = p\n", "        \n", "    # (step4) repeat steps 2-3 until convergence criteria met\n", "    if error < convergenceCriteria:\n", "        break\n", "    \n", "print('-------Final Centroids-------')\n", "print(np.array(centroids))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Results\n", "Examine the results by assigning each of the datapoints in the file to the nearest cluster and then plot all of the samples along with their labels."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["samples = np.loadtxt(IRIS_DATA, \n", "                     delimiter=',')\n", "labels = list(map(lambda sample: closestCluster(sample, centroids), \n", "             samples))\n", "plot_iris_data(np.array(samples), \n", "     np.array(labels), \n", "     ['cluster1', 'cluster2', 'cluster3'],\n", "     'Iris Dataset (Spark K-Means clustering)')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "462px", "width": "252px"}, "navigate_menu": true, "number_sections": true, "sideBar": true, "threshold": 4, "toc_cell": true, "toc_position": {"height": "567px", "left": "0px", "right": "707.4456787109375px", "top": "105px", "width": "243px"}, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 2}