{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 3 - Hadoop Shuffle & TOS\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2019`__\n",
    "\n",
    "Designing MapReduce algorithms involves two kinds of planning. First we have to figure out which parts of a calculation can be performed in parallel and which can't. Then we have to figure out how to put those pieces together so that the right information ends up in the right place at the right time. That's what the Hadoop shuffle is all about. Today we'll talk about a few techniques to optimize your Hadoop jobs. By the end of this demo you should be able to:  \n",
    "* ... __identify__ what makes the Hadoop Shuffle potentially costly.\n",
    "* ... __define__ local aggregation & identify when it won't help.\n",
    "* ... __implement__ partial, unordered, and total order sort.\n",
    "* ... __create__ custom counters for your Hadoop Jobs.\n",
    "* ... __describe__ the order inversion pattern & when to use it (ie: relative frequencies).\n",
    "\n",
    "**Note**: Hadoop Streaming syntax is very particular. Make sure to test your python scripts before passing them to the Hadoop job and pay careful attention to the order in which Hadoop job parameters are specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# imports & magic commands\n",
    "import sys\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\"\n",
    "HOME_DIR = \"/media/notebooks\" # this is where docker mounts your repo, ADJUST AS NEEDED\n",
    "DEMO_DIR = HOME_DIR + \"/LiveSessionMaterials/wk03Demo_HadoopShuffle\"\n",
    "HDFS_DIR = \"/user/root/demo3\"\n",
    "!hdfs dfs -mkdir {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store notebook environment path\n",
    "from os import environ\n",
    "PATH  = environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`REMINDER:`__ If you are running this notebook from the course Docker container you can track your Hadoop Jobs using the UI at: http://localhost:19888/jobhistory/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "In this notebook, we'll continue working with the  _Alice in Wonderland_ text file from HW1 and the test file we created for debugging. Run the following cell to confirm that you have access to these files and save their location to a global variable to use in your Hadoop Streaming jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data subfolder - RUN THIS CELL AS IS\n",
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Re)Download Alice Full text from Project Gutenberg - RUN THIS CELL AS IS \n",
    "# NOTE: feel free to replace 'curl' with 'wget' or equivalent command of your choice.\n",
    "!curl \"http://www.gutenberg.org/files/11/11-0.txt\" -o data/alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/alice_test.txt\n",
    "This is a small test file. This file is for a test.\n",
    "This small test file has two small lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# save the paths - RUN THIS CELL AS IS (if Option 1 failed)\n",
    "ALICE_TXT = PWD + \"/data/alice.txt\"\n",
    "TEST_TXT = PWD + \"/data/alice_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the files are there - RUN THIS CELL AS IS\n",
    "!echo \"######### alice.txt #########\"\n",
    "!head -n 6 {ALICE_TXT}\n",
    "!echo \"######### alice_test.txt #########\"\n",
    "!cat {TEST_TXT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load the file into HDFS for easy access__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input files into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal {TEST_TXT} {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal {ALICE_TXT} {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Overview: Hadoop Shuffle\n",
    "\n",
    "The week 3 reading from Chapter 3 of _Data Intensive Text Processing with Map Reduce_ by Lin and Dyer discusses 5 key techniques for controlling execution and managing the flow of data in MapReduce:\n",
    "1. using complex data structures to communicate partial results\n",
    "2. user-specified initialization/termination code before/after each map/reduce task\n",
    "3. preserving state across multiple keys\n",
    "4. controling the sort order of intermediate keys\n",
    "5. specifying the partitioning of the key space\n",
    "\n",
    "### Our goal in employing these techniques is to minimize the amount we have to move the data. This involves keeping track of what data is stored where at each stage in our job (_DITP figure 2.5 and 2.6_):\n",
    "*Recall that a Hadoop cluster stores our data on datanotes and ships the programmer's map and reduce code to those nodes to perform those transformations in place.*\n",
    "\n",
    "![HDFSdiagram](DITP_fig2-5,6.png)\n",
    "\n",
    "\n",
    "### Though it isn't always possible to do so, ideally we'd like to design an implementation that retrieves the result in a single MapReduce job (_DITP figure 2.4_):\n",
    "![MRdiagram](DITP_fig2-4.png)\n",
    "\n",
    "### Shuffle & Sort Detail (_Hadoop, The Definitve Guide, by Tom White; fig 7-4_):   \n",
    "![CircularBuffer-DFG](HDG_fig7-4-annotated.png)\n",
    "\n",
    "\n",
    "> __DISCUSSION QUESTIONS:__  \n",
    "> * What work does your Hadoop cluster have to do at the shuffle stage? \n",
    "> * What determines the time complexity of this work? \n",
    "> * Compare the Lyn & Dyer diagram with the om White diagram. How might the Lyn & Dyer diagram be misleading?\n",
    "> * What is a combiner, how does it impact the shuffle? 'where' does the combining happen?\n",
    "> * What is local aggregation? Is a combiner the only way to do it?\n",
    "> * What is a partitioner and how does it impact the shuffle? How/where can we specify custom paritioning behavior? If we don't specify a partitioner, will Hadoop still partition the data? How?\n",
    "> * Can you think of an example of a task that can't be accomplished in a single MapReduce job? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *_A note on the Lin and Dyer reading (and other readings you may encounter):_\n",
    "\n",
    ">In MapReduce, the programmer defines a mapper and a reducer with the following signatures:   \n",
    "map: (k1; v1) -> [(k2; v2)]   \n",
    "reduce: (k2; [v2]) -> [(k3; v3)]   \n",
    "where [..] denotes a list\n",
    "\n",
    "_It is important to note that the MapReduce framework ensures that the keys are ordered, so we know that if a key is different from the previous one, we have moved into a new key group. In contrast to the Java API, where you are provided an iterator over each key group (as per above Lin and Dyer version), in Streaming you have to find key group boundaries in your program. (Page 39, Hadoop - The Definitive Guide)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Relative Frequencies\n",
    "In last week's live session and HW1 we used Word Count as a cannonical example of an embarassingly parallel task. At first glance, computing relative frequencies (`word count / total count`) seems like it would be just as easy to implement -- after all it's just word count with an extra division at the end. However this task actually presents a small design challenge that is perfect to illustrate a few of the techniques that Lin & Dyer talk about.\n",
    "\n",
    "__DISCUSSION:__\n",
    "> * Talk through what the MapReduce job would look like? What is the challenge here?\n",
    "> * Is it possible to compute relative frequencies in a single MapReduce job? \n",
    "> * Is it possible to compute relative frequencies in a single MapReduce job with multiple reducers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Tasks:\n",
    "\n",
    "* __a) read provided code:__ We've provided a naive interpretation to get you started. Take a look at __`Frequencies/mapper.py`__, __`Frequencies/combiner.py`__  and __`Frequencies/reducer.py`__.  \n",
    "\n",
    "* __b) discuss:__ How does it resolve the challenge of computing the total? Uncomment line 22 in the mapper and lines 26-27 in the reducer to take advantage of this 'solution', then run the unit tests below, to confirm that you understand what's going on in each of these scripts. Despite solving the problem of computing the total in a single map-reduce job, what is wrong with this approach? \n",
    "\n",
    "* __c) fix the problem:__ To fix the problem, all we need to do is make a small change to the key used to emit the total counts (you need to do this in both the mapper and reducer). Think about Hadoop's default sorting. What keys arrive first? What key could you assign to the total that would be guaranteed to arrive first? Once you are satisfied that your solution works, run the provided Hadoop job on the test file & then the full `alice` text. (note it has a single reducer for now).  \n",
    "\n",
    "* __d) discuss:__  Now, re-run the job with 4 reducers, what happens to the results? What would we have to do to fix the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - make sure scripts are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x Frequencies/mapper.py\n",
    "!chmod a+x Frequencies/combiner.py\n",
    "!chmod a+x Frequencies/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - unit test mapper script\n",
    "!echo \"foo foo quux labs foo bar quux\" | Frequencies/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - unit test map-combine (sort mimics shuffle) (RUN THIS CELL AS IS)\n",
    "!echo \"foo foo quux labs foo bar quux\" | Frequencies/mapper.py | sort -k1,1 | Frequencies/combiner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - unit test map-combine-reduce (sort mimics shuffle) (RUN THIS CELL AS IS)\n",
    "!echo \"foo foo quux labs foo bar quux\" | Frequencies/mapper.py | sort -k1,1 | Frequencies/combiner.py | Frequencies/reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts c - clear the output directory (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/frequencies-output\n",
    "# NOTE: this directory won't exist unless you are re-running a job, that's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts c - Hadoop streaming job (RUN THIS CELL AS IS FIRST, then make your modification)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files Frequencies/reducer.py,Frequencies/mapper.py,Frequencies/combiner.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -combiner combiner.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/alice_test.txt \\\n",
    "  -output {HDFS_DIR}/frequencies-output \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the first few results (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/frequencies-output/part-00000 | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected Results:__\n",
    "\n",
    "<table>\n",
    "<th>part c (test)</th>\n",
    "<th>part c (full)</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "!total\t1.0\n",
    "a\t0.1\n",
    "file\t0.15\n",
    "for\t0.05\n",
    "has\t0.05\n",
    "is\t0.1\n",
    "lines\t0.05\n",
    "small\t0.15\n",
    "test\t0.15\n",
    "this\t0.15\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "!total\t1.0\n",
    "a\t0.0226802090523617\n",
    "abide\t6.57397363836571e-05\n",
    "able\t3.286986819182855e-05\n",
    "about\t0.0033527265555665124\n",
    "above\t9.860960457548565e-05\n",
    "absence\t3.286986819182855e-05\n",
    "absurd\t6.57397363836571e-05\n",
    "accept\t3.286986819182855e-05\n",
    "acceptance\t3.286986819182855e-05\n",
    "</pre></td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "__DISCUSSION:__  \n",
    "> *  Was the original implementation scalable? How does it resolve the challenge of computing the total? \n",
    "> * Despite solving that problem, what is wrong with this approach?  \n",
    "> * Did you come up with a new key that is guaranteed to arrive first? how?\n",
    "> * What happened when you went from 1 reducer to 4, why? (HINT: Use the UI to see the error logs for the failed reduce tasks)\n",
    "> * What do we need to do with the total counts when we move up to 4 reducers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Custom Partitioning\n",
    "\n",
    "Last week in Breakout 4 you learned out to implement a secondary sort -- that is, you learned how to tell Hadoop to order key-value pairs within each partition based on the value. However you also saw the limitation of this simple secondary sort: namely that sorting within a partition is not very useful if you need to use multiple reducers because, for example, the top word could end up in any one of the partitions and the next highest might end up in a totally different partition. Of course post processing your partially sorted partition files (eg. using mergesort) might solve this problem, but if your data is too large to fit on a single machine that is not a viable solution.\n",
    "\n",
    "Luckily, Hadoop provides a way to partition the data across reducers in a user-defined way.  This is done telling Hadoop to use all or part of the composite key as a partitioning key. All lines with the same partition key are guaranteed to be processed by the same reducer. This is similar to the sort key, but allows for control at a higher level. This \"custom partitioning\" will both solve our sorting troubles from last week and solve the problem you saw when using multiple reducers in Exercise 1 this week. To use custom partitioning, there are 2 more parameters we need to add to our Hadoop Streaming Command:\n",
    "\n",
    " __`-D mapreduce.partition.keypartitioner.options=\"-k1,1\"`__: tells Hadoop that we want to use the first key field as a partition key. Note: just like the `keycomparator`, `keypartitioner` must be used in conjuction with `stream.num.map.output.key.fields`.\n",
    "\n",
    "__`-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner`__: tells Hadoop that we want to partition the data in a custom way. Note about partitioner: this line **MUST** appear after all of the `-D` options or it will be ignored.\n",
    "\n",
    "> __DISCUSSION QUESTIONS (before exercise 2):__\n",
    "* Quick review: What is a composite key? \n",
    "* Quick review: Practically speaking, what is a 'partition'? How is this concept related to the HDFS output of a job?\n",
    "* What is the difference between the partition key & the 'sort' key?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Tasks:\n",
    "\n",
    "* __a) read provided code:__ In __`PartitionSort/mapper.py`__ we've provided a function that will assign a custom partition key (just a letter) to each word. We're going to use this mapper to sort our sample file with 3 partitions. Read this script. \n",
    "\n",
    "* __b) discus:__ How does the mapper decide which partition to assign each record? When you print out the results in what order do you expect to see the records?\n",
    "\n",
    "* __c) Hadoop job:__ Add the required parameters to complete the Hadoop Streaming code below. Your job should partition based on the newly added first key field, and sort alphabetically by word. Run your job. [__`Hint:`__ Don't forget to specify the number of fields!]\n",
    "\n",
    "* __d) discuss:__ Examine the output from your job. Compare it to the partitioning function we used. Are the words sorted alphabetically? What is suprising about the partition key that ended up in `part-00000`?\n",
    "\n",
    "* __e) code:__ If time permits, modify your job so that it sorts the words by _count_ instead (still using 3 partitions). To do this you will need to change the partition function in __`PartitionSort/mapper.py`__ so that it partitions based on count instead of the first letter of the word. Use 4 and 8 as your cut-points. You will also need to modify one of the Hadoop parameters (which one? why?). Run your job. Are you able to get a total sort? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile PartitionSort/sample.txt\n",
    "foo\t5\n",
    "quux\t9\n",
    "labs\t100\n",
    "bar\t5\n",
    "qi\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample file into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal PartitionSort/sample.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part a - complete your work above then RUN THIS CELL AS IS\n",
    "!chmod a+x PartitionSort/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts a - clear output directory (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/psort-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - Hadoop streaming command - ADD SORT and PARTITION PARAMETERS HERE\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  -files PartitionSort/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "\n",
    "  -input {HDFS_DIR}/sample.txt \\\n",
    "  -output {HDFS_DIR}/psort-output \\\n",
    "  -cmdenv PATH={PATH}\\\n",
    "  -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part a - Save results locally (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/psort-output/part-0000* > PartitionSort/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - view results (RUN THIS CELL AS IS)\n",
    "!head PartitionSort/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - look at first partition (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/psort-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - look at second partition (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/psort-output/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - look at third partition (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/psort-output/part-00002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected Output:__\n",
    "\n",
    "<table>\n",
    "<th>part c</th>\n",
    "<th>part e</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "B\tlabs\t100\n",
    "C\tqi\t1\n",
    "C\tquux\t9\n",
    "A\tbar\t5\n",
    "A\tfoo\t5\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "B\tlabs\t100\n",
    "B\tquux\t9\n",
    "C\tfoo\t5\n",
    "C\tbar\t5\n",
    "A\tqi\t1\n",
    "</pre></td>\n",
    "</tr></table>\n",
    "\n",
    "> __DISCUSSION QUESTIONS (exercise 2 debrief):__\n",
    "* In the provided implementation how did we assign records to partitions?\n",
    "* In part c, why didn't this partitioning result in alphabetically sorted words?\n",
    "* Given what you saw in part c, how did you 'trick' Hadoop into doing a full sort (by count) in part e?\n",
    "* If you didn't achive the full sort in `e` why not? On a larger data set what postprocessing would you have to do in this kind of scenario? Is this postprocessing non-trivial?\n",
    "* In addition to changing the partition function what other Hadoop parameter did you have to change for part `e`?\n",
    "* In the real world you wouldn't want your partition key as part of your output. What would we do to avoid this?\n",
    "* Does anyone need any additional clarification about any of the Hadoop Streaming?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Relative Frequencies Revisited\n",
    "### (+ intro to counters)\n",
    "\n",
    "Now that you know how to use custom partition keys, let's use this techinique to fix the problem from part d in Exercise 1.\n",
    "\n",
    "First a brief digression... A common challenge when implementing custom partitioning at scale is load balancing: _how can we ensure that each reducer node is doing approximately the same amount of work?_ Hadoop's option to define custom counters can be a useful way to monitor this kind of detail. Just like the built in  counters that you learned about last week, custom counters are a slight departure from statelessness. Normally we wouldn't want to share a mutable variable across multiple nodes, however in this case the framework manages so that you can increment them from any node without causing a race condition. \n",
    "\n",
    "To use a custom counter you'll just write an appropriately formatted line to the standard error stream. For example the following line would increment a counter called 'counter-name' in group 'MyWordCounters' by 10:\n",
    " > `sys.stderr.write(\"reporter:counter:MyWordCounters,counter-name,10\\n\")`\n",
    " \n",
    "This line can be added to your mapper, combiner or reducer scripts and wraped in `if` clauses to increment only under certain conditions. If a counter with that name/group doesn't exsit yet the framework will create one. The values of your custom counters will be printed to the console in their respective groups just like the built in Job Tracking counters. Counters can only be incremented using integers (ie, floats are not supported).\n",
    "\n",
    "Ok, armed with this new tool let's return to the relative frequencies task.\n",
    "\n",
    "__DISCUSSION:__\n",
    "> * Remember the problem we encountered at the end of Exercise 1? How would a custom partition key help us make sure the total counts get sent to each reducer node?\n",
    "> * What does this do to the number of records being shuffled?\n",
    "> * Where would you implement the custom partition key?\n",
    "> * How would you partition the records? (i.e what criteria would you use to assign keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Tasks:\n",
    "\n",
    "* __a) add a custom partitioner:__  In __`MultiPart/mapper.py`__, __`MultiPart/combiner.py`__, and __`MultiPart/reducer.py`__ we've copied the base code from the frequencies job. Complete the code in this mapper so that it adds a custom partition key to each record and emits the total subcounts _once for each partition_. Assume we'll be using 4 partitions. Then make any required adjustments to your reducer and combiner to accomodate the new record format (your final output should not include the partition key, though you may want it there initially for debugging purposes).\n",
    "\n",
    "* __b) discuss:__ Keep in mind that each partition still needs the total counts to arrive before the individual word counts. However since you've added a custom partition key to each record, the order inversion trick of adding a special character isn't going to work with Hadoop's default sorting behavior any more. Why not? What will you need to specify in your Hadoop job to make sure that the totals still arrive first?\n",
    "\n",
    "* __c) unit test & run your job:__ Write a few unit tests to debug your scripts. When you have them working as expected run the Hadoop job. [__`NOTE`__ We've provided a few tests to get you started but you'll need to add a unix sort to mimic the sorting you discussed in part 'b'].\n",
    "\n",
    "* __d) custom counters:__ Add custom counter(s) to your reducer code so that you can count how many records are processed by each reduce task. [__`TIP`__ use the partition key as the counter name and python3 string formatting to do this efficiently].\n",
    "\n",
    "* __e) discuss:__ Rerun your Hadoop job and take a look at the custom counter values. What do they tell you? Are these counts the same as the number of keys in the result of each partition? Try changing the partitioning criteria in your mapper... what partitioning results in a really uneven split? what partitioning results in the most even split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - make sure scripts are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x MultiPart/mapper.py\n",
    "!chmod a+x MultiPart/combiner.py\n",
    "!chmod a+x MultiPart/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - unit test mapper script\n",
    "!echo \"foo foo quux labs foo bar quux\" | MultiPart/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - unit test map-combine (ADJUST SORT AS NEEDED)\n",
    "!echo \"foo foo quux labs foo bar quux\" | MultiPart/mapper.py | sort -k1,1 | MultiPart/combiner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - unit test map-combine-reduce (ADJUST SORT AS NEEDED)\n",
    "!echo \"foo foo quux labs foo bar quux\" | MultiPart/mapper.py | sort -k1,1 | MultiPart/combiner.py | MultiPart/reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts c - clear the output directory (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/multipart-output\n",
    "# NOTE: this directory won't exist unless you are re-running a job, that's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - Hadoop streaming command - FILL IN HERE (don't forget to specify 4 reducers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  part c - take a look at a few records from each partition (RUN THIS CELL AS IS)\n",
    "for p in range(4):\n",
    "    print('='*10,f'PARTITION{p+1}','='*10)\n",
    "    !hdfs dfs -cat {HDFS_DIR}/multipart-output/part-0000{p} | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected Results:__  \n",
    "__`NOTE:`__ exact partition splits depend on your custom function:\n",
    "\n",
    "Test file:\n",
    "<table>\n",
    "<th>partition 1</th>\n",
    "<th>partition 2</th>\n",
    "<th>partition 3</th>\n",
    "<th>partition 4</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "test\t0.15\n",
    "this\t0.15\n",
    "two\t0.05\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "a\t0.1\n",
    "file\t0.15\n",
    "for\t0.05\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "has\t0.05\n",
    "is\t0.1\n",
    "lines\t0.05\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "small\t0.15\n",
    "</pre></td>\n",
    "</tr></table>\n",
    "\n",
    "Full Alice Text:\n",
    "<table>\n",
    "<th>partition 1</th>\n",
    "<th>partition 2</th>\n",
    "<th>partition 3</th>\n",
    "<th>partition 4</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "t\t0.00716563\n",
    "table\t0.00059165\n",
    "tail\t0.00029582\n",
    "tails\t9.8609e-05\n",
    "take\t0.00072313\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "a\t0.022680209\n",
    "abide\t6.5739e-05\n",
    "about\t0.00174210\n",
    "able\t3.2869e-05\n",
    "above\t3.2869e-05\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "gained\t3.2869e-05\n",
    "gallons\t3.2869e-05\n",
    "game\t0.00042730\n",
    "games\t3.2869e-05\n",
    "garden\t0.00052591\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "name\t0.00036156\n",
    "named\t3.2869e-05\n",
    "names\t6.5739e-05\n",
    "narrow\t6.5739e-05\n",
    "natural\t3.2869e-05\n",
    "</pre></td>\n",
    "</tr></table>\n",
    "\n",
    ">__DISCUSSION__\n",
    "> * After adding the custom partition key what else did you have to do so that the order inversion pattern would still work?\n",
    "> * In part `d` what did you see from your custom counters? Do these numbers match the number of keys in the result of each partition? Why/why not?\n",
    "> * What other partitioning cuts did you try? What partitioning results in a really uneven split? what partitioning results in the most even split?\n",
    "> * Do custom counters solve the load balancing challenge?\n",
    "> * If we wanted to design a subsequent job to sort these results from highest to lowest frequency what partitioning strategy would you explore? Any particular challenge there? (__`HINT:`__ you may wish to consider the kinds of numbers you saw in the results from Exercise 1, how python stores floats, and what you know about word frequencies in natural language)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Unordered Total Sort vs Total Order Sort\n",
    "\n",
    "In HW1 we emphasized that sorting can be a useful preprocessing tool because sorted input can sometimes facilitate a more efficient algorithm design. However sorting is also often desirable in its own right. For example suppose you wanted to get the top and bottom 100 words according to their relative frequencies? The best way to get these results would be to do a Total Order Sort -- that is to sort the entire data set from highest to lowest so that you can simply read the first 100 records in the first partition and last 100 records in the last partition.\n",
    "\n",
    "So far we've tried two different ways of sorting using the Hadoop framework and neither quite achieved this result to satisfaction. First, in last week's breakout we performed a secondary sort using the `keycomparator` and accompanying Hadoop parameters. When we used a single reducer that strategy _did_ successfully result in a sort from top to bottom, but in a class all about scaling up for large data solutions that require a single reducer won't get us very far. Unfortunately when we tried using those same three sorting parameters and with multiple reducers we ended up with results sorted within each partition but not across partitions. This is what we call that a _partial sort_ because records are only ordered relative to the other keys that happen to be processed by the same reducer. Importantly a partial sort is of no use at all for us if we wanted to find the top 100 and bottom 100 from our relative frequencies file.\n",
    "\n",
    "Then in Exercise 2 of this notebook we learned how to specify a custom partition key so that we can explicitly control the partitioning. By combining the use of a custom partition key and the secondary sort from last week we were able to ensure that our results were not only sorted within their paritions but also that the paritioning grouped records with similar value ranges together. In theory this should have solved our 'total sort with multiple reducers' challenge but in practice something odd happened: the records with the top value didn't reliably end up in the first partition nor did the records with the lowest values necessarily end up in the last. The partition files themselves were out of order, making this an _unordered total sort_.\n",
    "\n",
    "The reason that the partitions appear out of order has to do with the hash function that Hadoop applies to your custom partition keys. That hash function results in an ordering that may not match the human readible string or integer numbering you as a programmer might have intended. Luckily, this hash function is both fixed and known -- so for example, if you are using partition keys `A`, `B`, and `C` your partitions will always end up ordered `B` - `C` -`A` (as you saw in Exercise 2). That means if we know in advance how many partitions (i.e. reducers) we plan to use, we can reverse engineer the hash function to figure out how Hadoop will order those keys and plan accordingly (for example in our 3 partition case by specifying that the top values should get the partition key `B` not `A`). When your data is sorted not only within each partition but the partitions are also in the right order, you've achieved a _total order sort_. \n",
    "\n",
    "Let's give it a try! Your job in this exercise is to write a Hadoop Job that will perform a total order sort on the output of Exercise 1 (relative frequencies) with any number of partitions.\n",
    "\n",
    "__Note__: Part III.D.4 in the [Total Sort Notebook](https://github.com/UCB-w261/main/blob/master/HelpfulResources/TotalSortGuide/_total-sort-guide-spark2.01-JAN27-2017.ipynb) has a much more comprehensive explanation that you may wish to reference here (ignore the MRJob code).\n",
    "\n",
    "/user/root/demo3/frequencies-output\n",
    "\n",
    "### Exercise 4 Tasks\n",
    "* __a) discuss:__ In the `%%writefile` cell below we create a partition file (__`partitions.txt`__) which contains the cut points we'll use to partition the data. Based on your reading of this file, how many partitions will we use? Think about the kinds of values that we got in Exercise 1 ... can you see any potential problems with these cutpoints?\n",
    "\n",
    "\n",
    "* __b) discuss:__ Read through __`TotalOrderSort/mapper.py`__ and __`TotalOrderSort/reducer.py`__. Which of these scripts makes use of our partition file? What does the mapper do? What does the reducer do? \n",
    "\n",
    "* __c) code:__ Run the provided code below to apply this mapper and reducer. Note how the Hadoop job reads directly from the output directory we created in Exercise 1 (you will need to be those results were computed inorder to run this job). Use the provided code to view the output and confirm that the current implementation performs an unordered sort. What adjustments might you want to make to the partition file? (go ahead and modify it as desired). Then modify the Hadoop job so that it uses `/bin/cat` instead of `reducer.py`... and re-run the job this will allow you to see the order in which the partition keys were sorted. \n",
    "\n",
    "* __d) discuss:__ Read through __`TotalOrderSort/TOS_mapper.py`__. Pay particular attention to the helper function `makeKeyHash()` -- what does this function do? how is it used? how is this mapper different that the original one? \n",
    "\n",
    "* __e) code + discussion:__ Replace the original mapper with this new mapper(_don't forget to add it to the `-files` line_) and rerun the job (still with `/bin/cat` as reducer). How did the parition ordering change? What happpens if you change the partition file so that it has one extra number? Does your job work? Re-place the true reducer. Et voila, you have achieved total order sort!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile TotalOrderSort/partitions.txt\n",
    "0.01,0.005,0.003,0.002,0.001,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - make sure files are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x TotalOrderSort/mapper.py\n",
    "!chmod a+x TotalOrderSort/reducer.py\n",
    "!chmod a+x TotalOrderSort/TOS_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - hadoop job (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/tos-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3nr\" \\\n",
    "  -files TotalOrderSort/mapper.py,TotalOrderSort/reducer.py,TotalOrderSort/partitions.txt \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/frequencies-output \\\n",
    "  -output {HDFS_DIR}/tos-output \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - print top words in each class (RUN THIS CELL AS IS)\n",
    "for idx in range(6):\n",
    "    print(f\"\\n============== PART-0000{idx}===============\")\n",
    "    numLines = !hdfs dfs -cat {HDFS_DIR}/tos-output/part-0000{idx} | wc -l\n",
    "    print(f\"Number of lines processed by this reducer: {numLines[0]}\\n\")\n",
    "    !hdfs dfs -cat {HDFS_DIR}/tos-output/part-0000{idx} | head | column -t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
